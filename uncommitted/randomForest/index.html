<!DOCTYPE html>
<html lang="en">
 <head>
  <meta charset="utf-8"/>
  <meta content="width=device-width, intial-scale=1" name="viewport"/>
  <!-- Name on tab -->
  <title>
   Random Forests
  </title>
  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Cormorant+Garamond:400,700&display=swap" rel="stylesheet"/>
  <!-- Custom CSS -->
  <link crossorigin="anonymous" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" rel="stylesheet"/>
  <link href="../../css/style.css" rel="stylesheet"/> <!--TARGET-->
  <link href="../../css/pygments.css" rel="stylesheet"/> <!--TARGET-->
  <script src="https://cdn.jsdelivr.net/npm/p5@0.10.2/lib/p5.js">
  </script>
  <!-- Custom p5js animations. Add any other .js here -->
  <script src="sketch.js">
  </script>
  <script src="tree.js">
  </script>
 </head>
 <body>
  <!-- Navigation bar -->
  <nav class="navbar navbar-expand-lg navbar-dark fixed-top navbar-custom">
   <a class="navbar-brand" href="../.." style="font-weight:bold;"> <!--TARGET-->
    Ariel Yssou
   </a>
   <button aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation" class="navbar-toggler" data-target="#navbarSupportedContent" data-toggle="collapse" type="button">
    <span class="navbar-toggler-icon">
    </span>
   </button>
   <div class="collapse navbar-collapse" id="navbarSupportedContent">
    <ul class="navbar-nav mr-auto">
     <li class="nav-item">
      <a class="nav-link" href="../.." style="font-weight:bold;"> <!--TARGET--> 
       Home
      </a>
     </li>
     <li class="nav-item nav-blog active">
      <a class="nav-link" href="../../blog" style="font-weight:bold;"> <!--TARGET--> 
       Blog
       <span class="sr-only">
        (current)
       </span>
      </a>
     </li>
     <li class="nav-item">
      <a class="nav-link" href="../../gallery" style="font-weight:bold;"> <!--TARGET--> 
       Gallery
      </a>
     </li>
     <li class="nav-item">
      <a class="nav-link" href="../../about" style="font-weight:bold;"> <!--TARGET--> 
       About
      </a>
     </li>
    </ul>
   </div>
   <div class="collapse navbar-collapse" id="navbarSupportedContent">
    <ul class="navbar-nav ml-auto">
     <li class="nav-item">
      <a class="nav-link text-nowrap" href="https://github.com/ArielYssou" target="_blank">
       <i class="fab fa-github fa-lg">
       </i>
      </a>
     </li>
     <li class="nav-item">
      <a class="nav-link text-nowrap" href="https://www.linkedin.com/in/ariel-yssou-oliveira-f-2a07a5b5/" target="_blank">
       <i class="fab fa-linkedin fa-lg">
       </i>
      </a>
     </li>
    </ul>
   </div>
  </nav>
  <!-- End of navigation bar -->
  <div class="head_container">
   <img class="head_image no_blur" src="./cover_image.png"/>
   <div class="head_middle">
    <div class="head_text">
     <b>
      Random Forests
     </b>
    </div>
   </div>
  </div>
  <div class="container post_body">
   <div class="row post_struct">
    <div class="col-ld-12">
     <p>
      INTRO TEXT OF
      <b>
       Random Forests
      </b>
      .
     </p>
     <!-- TOC -->
     <table class="TOC">
      <tr>
       <th>
        Contents
       </th>
      </tr>
      <tr>
       <td>
        <a href="#INTRO">
         1. Introduction
        </a>
       </td>
      </tr>
      <tr>
       <td>
        <a href="#DECISION TREES">
         2. Decision trees
        </a>
       </td>
      </tr>
      <tr>
       <td>
        <a href="#Random Forests">
         2. Random Forests
        </a>
       </td>
      </tr>
      <tr>
       <td>
        <a href="#CODE">
         3. Applications
        </a>
       </td>
      </tr>
     </table>
     <br/>
     <a name="#INTRO">
     </a>
     <h2 class="section">
      1. Introduction
     </h2>

		 Random forest are one of the most famous and common models in Machine learning as they are easy to implement, very powerful, adaptable and are fast to train. Most texts about these models are focused on the practical implementation of random forests but usually lack a deeper view into the statistics that provide such excellent results. This post will be focused on providing a more solid view into the theory of these models and study why they are so powerful and their potential shortcomings.

		 The text is structured as follows. Firstly we will discus <b>Decision Trees</b>, which are the base models that compose a random forests. With this formalism we will be able to discuss the theory of Random forests in chapter 2. The last chapter will be dedicated to the implementation of RFs in python using the XXXX dataset.

			<a name="DECISION TREES">
      </a>
      <h2 class="section">
       2. Decision trees
      </h2>
			A decision tree is a predictive modeling technique that make predictions based on a hierarchy of decisions, which can be used both for classification or regression purposes. The simplest and most common type of decision tree is the <i>Ordinary binary classification trees</i> (OBST), which  has the objective of attributing a class label to each input  based on a set binary questions (yes or no questions). 

			To illustrate the overall architecture of a decision tree, lets consider the task of classifying an population on two Lets consider an example illustrating a decision tree architecture on the classification of a population can be represented as follows:

			<!-- Image 1: Separando uma população em duas classes com perguntas -->

			The tree is then consists of a set of <b>nodes</b> that split the population. The algorithm starts at the root node where the entire population is splitted, and then follows the node branches until a classification is made at the last node, called a "leaf".

			<!-- Scheme of a tree -->
			

			Decision trees are one of the simplest models available as they consist of a simple hierarchy of questions. Albeit they usually wield lest than ideal results (in terms of precision) they are still widely used as their simple formulation allows for the complete interpretation of the model. This characteristics can be very desirable as in some practical cases when there is a high risk associated with the usage of the model and the pratictioner cannot risk everything in a black box model.

			<p>The most common type of decision trees are , which split the feature space via a sequence of decisions, each base on a single feature and are in the form of "is feature $i$ greater that 1/2?". Depending on the answer (either yes or no). The population will be splitted until a criteria is met and the algorithm is stopped. In the end the population will be separated in different groups, each on a leaf of the tree. I the objective of the tree is a classification the tree will atributed a class to each leaf and if the objective is regression then the target variable will be estimated as the mean of the population of each leaf.</p>

			TO further increase our undertanding of decision trees lets consider the following scenario. Say that we have a mixed population of Hobbits (class $\omega_1$) and Dwarfs 9class $\omega_2$) From each individual we know only 3 features: height, weight and age. From the lore, Hobbit heights range from 0.9 to 1.2 meters while dawarfs are 1.2 to 1.5m tall.

			The basic idea behind OBCT is to sequentially split the feature space into regions corresponding to different classes, as represented in the following animation:

			<!-- Example of a decision tree -->

			the thask illustrated in the example above is in a two dimensional space wich has clear geometric splits. This, however, is usually not the case in practical cases where the feature space is very high dimensional and a geometric approach cannot be achieved. In order to generalize the algorithm, we need to establish the following elements in the training phase:
			<ol>
				<li>Candidate questions: At each node the set of questions that can be asked has to decided. Each question represents a specific binary split into two descendant nodes. Each node $t$ is associated with a specific subset $X_t$ of the full training set $X$. The split at node $t$ separates the population $X_T$ into two subsets , $X_{t,+}$ and $X_{t,-}$, that correspond to the positive and negative answer to the question (yes and no respectively). Thus the following is true:
					$$X_{t,+} \bigcap X_{t,-} = \emptyset$$
					$$X_{t,+} \bigcup X_{t,-} = X_t$$
				</li>
				<li>Splitting criterion: To select the best split at each node we must choose a criterion.</li>
				<li>Stop splitting criterion: To control the growth of the tree a stopping condition must be set, otherwise the tree would grown indefinitely;</li>
				<li>Class assignment rule: We must set a rule to assign a class to each leaf</li>
			</ol>
			Now we are now read to delve into each of the elements above.
			 <h3>
				 Candidate Questions
			 </h3>
			 For the binary classification type of trees, the quests are in the form "Is feature $x_k \leq \alpha$?". For each feature, each value of $alpha$ represents a different split of the subset $X_t$. If $X_k$ is a numerical variable, then the number of possible $\alpha$ values is infinite. In practice we can only consider a finite set of questions. Most algorithms choose the set of thresholds $\alpha$ considering the halfway between all consecrates distinct values of $x_k$ in the population $X_t$

			 This type of question leads to linear segmentation of the feature hyperspace. More complex questions can lead to different kinds of splits, such as "Is features $X_k^2 < 5$, which represents circular splis in the feaures space.
			 <h3>
				 Split criterion
			 </h3>
			 Each split generates two descendant nodes, each with populations $X_{t,+}$ and $X_{t,-}$. But how can we determine which split is "best"? It depends on what we want to achieve with the algorithm. It makes sense to split the population into groups that are more "class homogenous" than the ancestor subset $X_t$. the goal is then to define a metric that quantifies impurity or homogeneity.
			<h4>
				Entropy
			</h4>
			 In information theory there is the notion of impurity which is based on Shannon entropy. While the term "entropy" was borrowed from physics, it doesn't have the same exact interpretation, although there are some conceptual semblances between then.
			 In physics entropy is related to the general disorder of a system, It has many interesting properties both in classical and in quantum physics. If you are interested in a more deep understanding of this topic I've made a brief introduction to it bellow, but feel free to skip it if you just want to know the information theory approach.
			 In information theory entropy has a different meaning. The idea is to create a quantity (called as entropy by Shannon) to measure how much information is created by a process. The amount of information in a message or processes can be quantified by the amount of <b>surprise</b> it generates (here surprise is the correct technical term for the thing, believe it or not). A message that brings no new information generates no surprise in the reader, thus generating no entropy.
			 Lets define a <b>surprise function</b> as $I(p)$, where $p$ is an event (thus $I(P_n)$ is the amount of surprise generated when the state of the system is equal to $x=x_n$. The total entropy is thus given by the average  information gained from each individual state weighted by the probability $P_n$ of said event happening:
			 $$S = \sum_n p_n I(p_n)$$
			 to correctly measure information, the function$I_n$ should have some desirable properties (as stated by Shannon in his original study):
			 <ol>
				 <li>$I(p) \geq 0$ Information must be a positive quantity.</li>
				 <li>$I(1) = 0$: If a event is certain $p=1$, then no surprise can come from it.</li>
				 <li>$I(p)$ should decrease monotonically in $p$. Events of higher probability  yield lower surprises.</li>
				 <li>$I(p_1 p2) = I(p_1) + I(p_2)$ The information gained from two statistically independent events is the sum of the gain from each individual event.</li>
			 </ol>
			The only function to satisfy all conditions, specially condition 4, is the logarithm (for a more thought demonstration see the original paper):
			$$I(p) = - K \ln(p),$$
			where $K$ is a positive constant. The minus comes from condition 1, as $p\leq 1$ then the constant must be negative to keep the whole function positive. The log can be in any base as changing it would only affect the constant $a$. Finally we have that the (Shannon) entropy can be written as:
			$$\boxed{S = -K\sum p_n \ln(p_n)}.$$
			In summary, the entropy in information theory quantifies the amount of information in $P_n$ as it a measure of the average surprise $\langle - \ln(p_n) \rangle$ from the transition to state $x_n$.
			Thus a decision forest can split a population based on the total entropy of each subset. In practice probabilities are estimated via the observed frequency of points belonging to each class.
			<h4>
				Gini Impurity
			</h4>
			Lets consider a population of $N$ classes, where the observed frequency of each class $i$ (i.e. the percentage of elements belonging to each class) is $pa_i$. The Gini impurity is the chance of mislabeling a random element of the population based on the observed frequencies $p_i$. If a population is "pure" (only has 1 class), then the impurity is 0 as it cannot be mislabeled. If there is the same number of elements on each class then the impurity is high as there is a $(N-1)/N$ probability of incorrectly labeling an element.
			Formally the probability of mislabeling an element with label $i$ is $\sum_{k \neq i}p_k = 1 - p_i$. Thus we can write de Gini inpurity $I_G$ as
			$$I_G = \sum_{i=1}^N (probability\ of\ selecting\ label\ i) \times (probability\ of\ mislabel\ i)$$

			$$I_G = \sum_{i=1}^N p_i (1-p_i) = \sum_{i=1}^N p_i - \sum_{i=1}^N p_i^2 = 1 - \sum_{i=1}^N p_i^2$$
			The Gini impurity has a interesting correlation with the Shannon entropy as both of them are special cases of the Tsallis entropy
			$$S_q(p_i) = \frac{k}{q-1} \langle 1 - \sum_i p_i^q \rangle),$$
			where the Shannon entropy can be obtained with $q=1$ and the Giny Inpurity with $q=2$. Although the actual physical meaning of the Tsallis Entropy is debatable (as it is supposedly an entropy that is not addictive), this relation between both measures of impurity is very cool nevertheless.

			<h4>
				Measure of "goodness"
			</h4>


			<h4>
				Variance Reduction
			</h4>
			In regression tasks, we don't seek to attribute labels to elements, but continuous values (regression trees), thus our previous metrics can't be directly applied (not without performing a discretization of the variables, which may be inappropriate for regression purposes). One such metric is Variance Reduction $I_V(N)$, which consists of the difference between the variance in the parent node and its childs:
			$$I_V(N) = Var(Y) - (Y_t) + Var(Y_f),$$
			where Y, Y_t, Y_f are the set of target variables of the parent, left child and right child nodes and
			$$var(Y) \equiv  \frac{1}{N} \sum_{i=1}^N \sum_{j=1]^N \frac{1}{2} (y_i - y_j)$$

			<h3>
				 Stop Splitting Rule
			</h3>
			As the tree will recursively realize splits based on the metrics discussed, it needs to stop at some time. A sensible choice is to set a threshold on the split metric: if the maximum value of $\Delta I(t)$ exceed the threshold, then the iteration is stopped; but there are other options such as stop splitting when the cardinality of the subset $X_t$ is small or stop only when each subnode is "pure" (contains only elements of a single class).

			<h3>
				 Class Assignment Rule
			 </h3>
			 Perhaps the easiest step, to assign a class to a given node we can just use the class which is more common in that node. More formally the elements of a node will be assigned to class $i$ if $$arg\ max_i P(\omega_i|t).$$
			<h3>
				 Full Algorithm
			 </h3>
			 We are now in position to build an outline of the full binary classification tree algorithm. The outline can be written as follows:
			 <ol>
				 <li>
					 Start: Build a root node and pass it the entire population
				 </li>
				 <li>
					 For each node t:
					 <ol>
						 <li>For each feature $x_k$:
							 <ol> 
								 <li>
									 For each candidate value $alpha_{k,n}$, with $n=1,2,3,4,\dots, N_{t,k}$:
									 <ol>
										 <li>
											 Ask "Is $x_k \leq \alpha_{kn}$" and split the population into the subgroups $X_{t,+}$ and $X_{t,-}$ for the elemtens corresponding to the positive and negative outcome of the split question.
										 </li>
										 <li>
											 Compute the impurity decrease (there are several options for this metric)
										 </li>
									 </ol>
								 </li>
								 <li>
									 End
								 </li>
							 </ol>
						 </li>
						 <li>
							 Choose $x_{k*}$ (and associated $\alpha_{k*,n*}$) according to the split that lead to the overall maximum decrease in impurity.
						 </li>
						 <li>
							 If the stop splitting creterion is met, set the current node as a leaf and apply the class assignment rule to set the class label of the elements of $$X_t$.
						 </li>
						 <li>
							 Else, generate two child nodes with associated subsets $X_{t,+}$ and $X_{t,-}$ depending to the outcome of the question $X_{k*} \leq \alpha_{k*n*}$
						 </li>
					 </ol>
				 </li>
			 </ol>

			<a name="Random Forests">
      </a>
      <h2 class="section">
       2. Random Forests
      </h2>

			The idea behind random forests is to combine several decision trees that outperform each individual tree. This performance gain is due to the combination of several techniques, not by simply strapping decision trees together and hopping for the best. It was Leo Breiman that combined the ideas of bagging (bootstrap aggregation), stochastic learning and subspace selection to create the algorithm for random forests. He also introduced some novel insights such as estimating the generalization error as the out-of-bag (OOB) error and estimating the importance of each individual feature via permutation. We will now delve into each one of these ingredients and see how they help the random forest algorithm to outperform single CARTs.

			<h3>
				 Stochastic Learning
			</h3>
				One major shortcoming of decision trees is their susceptibility to overfitting. Trees are, by themselves, not very good in terms of precision and need to grow in complexity to increase their performance. This need for complexity can easily lead to overfitting and poor generalization. Kleinbergs theory of stochastic discrimination (SD) showed that the combination of weak learners could not only be arbitrarily precise, but also fast, stable and resistant to overfitting .
				The stochastic discrimination process is defined as follows. Consider the two class problem with an initial population of $N$ elements. We also have an set of $S$ possible solutions, in our case each member of S is an week learner (weak here means just that the algorithm can be just slightly better than random guessing). We can think of the sequence of solutions as items appearing over time, and see the transitions of a given element between solutions as an stochastic process. It can be shown that for large numbers of solutions the average of the stochastic processes tend to two distinct limits, thus allowing for a more robust discrimination, as represented in the following scheme (an direct adaptation from the original scheme in Kleinbergs article):
				The fact that the stochastic process not only stably converge (on average over large sets of possible solutions) but also in polinomial time are very powerful and intriguing results from SD theory, whose demonstration is outside the scope of this article, but can be seen in Kleinberg original work, while the resistance to overfitting is directly addressed in a separate article "".

			<h3>
				 Bagging
			</h3>
			Perhaps the biggest limitation of decision trees is their relative poor performance and high variance. One way to improve the overall quality of a model is to use <b>bagging</b>, which simply put is to train the model several times, but over subsets of the entire training set and combine all fits to generate predictions.
			Consider we have a training set $X$ and associated targets $Y$. Bagging would consist of sampling with replacement both $X$ and $Y$ $n$ times, generating subsets $X_i$, $Y_i$, $i=1,2,\dots,n$. Then a model is fitted on each subset $i$. The final prediction would then be the avarage over all $n$ models or by the majority vote.
			This methods reduces the overall variance of the week learners and also is more resistant to noises in the training set. However, if one cannot generate many statistically independent trees (due to a lack of training points, for example), not much would be achieved by this method as all models would be almost identical and the variance would not be reduced.
			Random forests however do not only employ bagging to the training set, but also for the features themselves. Each model will be presented with only some of the features available, which leads to decision trees that are uncorrelated. This idea was initially presented by Hu to formulate Decision Forests (not to be confused with random forests), which is an algorithm that only employs feature bagging.
			The major advantage of hiding some features from the models is that if there is a feature that has a very strong correlation to the target variable then all trees would tend to heavily depend on it for their predictions, making all trees almost identical as all decisions would be made solely on this single feature, thus making all trees strongly correlated and reducing the gains from bagging.
			Random forests as formulated bu Breiman use both types of bagging. Each tree is trained not only in a subspace of the full feature hyperspace, but also presented with only a fraction of all observations available in the training set.

			<a name="CODE">
      </a>
      <h2 class="section">
       3. Applications
      </h2>
      <p>
       Code section
      </p>
       <span class="alert">
        Click on the panel bellow to start/stop the simulation.
       </span>
       (In some browsers is necessary to click twice to unpause because I screwed the html in some unknown way)
      </p>
      <div class="sketch-holder" id="c1">
      </div>
      <p>
       Other canvas
      </p>
      <div class="sketch-holder" id="c2">
      </div>

      <p>
       Code:
      </p>
<pre>
{% include 'sketch.js' %}
</pre>

    </div>
   </div>
  </div>
  <hr/>
  <script src="../../js/script.js"> <!--TARGET-->
  </script>
  <!-- Bootstrap + JQuery -->
  <script crossorigin="anonymous" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" src="https://code.jquery.com/jquery-3.3.1.slim.min.js">
  </script>
  <script crossorigin="anonymous" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js">
  </script>
  <script crossorigin="anonymous" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js">
  </script>
  <script crossorigin="anonymous" src="https://kit.fontawesome.com/9ead9d8df4.js">
  </script>
  <!-- mathjax -->
  <script async="" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6">
  </script>
  <script>
   MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        svg: {
          fontCache: 'global'
        }
      };
  </script>
 </body>
</html>

