<!DOCTYPE html>
<html lang="en">
 <head>
  <meta charset="utf-8"/>
  <meta content="width=device-width, intial-scale=1" name="viewport"/>
  <!-- Name on tab -->
  <title>
   Random Forests
  </title>
  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Cormorant+Garamond:400,700&display=swap" rel="stylesheet"/>
  <!-- Custom CSS -->
  <link crossorigin="anonymous" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" rel="stylesheet"/>
  <link href="../../css/style.css" rel="stylesheet"/> <!--TARGET-->
  <link href="../../css/pygments.css" rel="stylesheet"/> <!--TARGET-->
	<!-- Load p5.js -->
  <script src="https://cdn.jsdelivr.net/npm/p5@0.10.2/lib/p5.js">
  </script>
	<!-- Load d3.js -->
	<script src="https://d3js.org/d3.v4.js"></script>
  <!-- Custom p5js animations. Add any other .js here -->
  <script src="sketch.js">
  </script>
  <script src="tree.js">
  </script>

 </head>
 <body>
  <!-- Navigation bar -->
  <nav class="navbar navbar-expand-lg navbar-dark fixed-top navbar-custom">
   <a class="navbar-brand" href="../.." style="font-weight:bold;"> <!--TARGET-->
    Ariel Yssou
   </a>
   <button aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation" class="navbar-toggler" data-target="#navbarSupportedContent" data-toggle="collapse" type="button">
    <span class="navbar-toggler-icon">
    </span>
   </button>
   <div class="collapse navbar-collapse" id="navbarSupportedContent">
    <ul class="navbar-nav mr-auto">
     <li class="nav-item">
      <a class="nav-link" href="../.." style="font-weight:bold;"> <!--TARGET--> 
       Home
      </a>
     </li>
     <li class="nav-item nav-blog active">
      <a class="nav-link" href="../../blog" style="font-weight:bold;"> <!--TARGET--> 
       Blog
       <span class="sr-only">
        (current)
       </span>
      </a>
     </li>
     <li class="nav-item">
      <a class="nav-link" href="../../gallery" style="font-weight:bold;"> <!--TARGET--> 
       Gallery
      </a>
     </li>
     <li class="nav-item">
      <a class="nav-link" href="../../about" style="font-weight:bold;"> <!--TARGET--> 
       About
      </a>
     </li>
    </ul>
   </div>
   <div class="collapse navbar-collapse" id="navbarSupportedContent">
    <ul class="navbar-nav ml-auto">
     <li class="nav-item">
      <a class="nav-link text-nowrap" href="https://github.com/ArielYssou" target="_blank">
       <i class="fab fa-github fa-lg">
       </i>
      </a>
     </li>
     <li class="nav-item">
      <a class="nav-link text-nowrap" href="https://www.linkedin.com/in/ariel-yssou-oliveira-f-2a07a5b5/" target="_blank">
       <i class="fab fa-linkedin fa-lg">
       </i>
      </a>
     </li>
    </ul>
   </div>
  </nav>
  <!-- End of navigation bar -->
  <div class="head_container">
   <img class="head_image no_blur" src="./cover_image.png"/>
   <div class="head_middle">
    <div class="head_text">
     <b>
      Random Forests
     </b>
    </div>
   </div>
  </div>
  <div class="container post_body">
   <div class="row post_struct">
    <div class="col-ld-12">

		<!-- Intro -->
		 <p>
		 	Random forest are one of the most famous and commonly used models in Machine learning as they are easy to implement, adaptable, offer excellent performance and are fast to train. It is fairly common to find many posts praising this model and teaching how to used it using modern machine learning libraries, but few texts go into much detail of the underling theory of random forests. This post is a result of my studies into the subject and it will hopefully provide some insights into the complexities of random forests that would otherwise only be found in technical books or in specific papers.
		 </p>
		 <p>
			Topics such as modern variations of both random forests and decision trees (which are the building blocks of random forests), methods for estimating the feature importances, susceptibility to overfitting and possible limitations and weaknesses will also be discussed to provide a more complete overview of not only the model itself but the techniques that are commonly used with it. 
		 </p>
		 <p>
		 	The text is structured as follows. Firstly we will discus the basic theory of <b>Decision Trees</b>. With this formalism we will be able to discuss the proper Random forests (RF) in Chapter 2. The third and last chapter will be dedicated to a simple and (hopefully) illustrative implementation of RFs in python using the XXXX dataset, with an emphasis on explaining how to extract a feature importance estimation of the trained model.
		 </p>
     <!-- TOC -->
     <table class="TOC">
      <tr>
       <th>
        Contents
       </th>
      </tr>
      <tr>
       <td>
        <a href="#DECISION TREES">
         1. Decision trees
        </a>
       </td>
      </tr>
      <tr>
       <td>
        <a href="#Random Forests">
         2. Random Forests
        </a>
       </td>
      </tr>
      <tr>
       <td>
        <a href="#CODE">
         3. Applications
        </a>
       </td>
      </tr>
     </table>
     <br/>
     <a name="#INTRO">
     </a>

			Obs: I have implemented most of the visualizations using the D3 framework, so many of then will be interactible and/or animated! Figures with the ... symbol are intractable so have fun exploring them!
			For the sake of meta (and implementing visualizations of no practical use) the following sketch was created using p5.js and is generated randomly (a random forest if you will). Each time you load this page a different set of trees will be printed.
			<div class="sketch-holder" id="c1">
      </div>
			<a name="DECISION TREES">
      </a>
      <h2 class="section">
       1. Decision trees
      </h2>

			<p>
				Decision trees are a class of models whose predictions are based on a hierarchy of decisions, which can be used both for classification or regression purposes. Due to their structural simplicity these models are fully interpretable and can provide some valuable insights about the underliing structure of the data. The simplest and most common type of decision tree is the <i>Ordinary binary classification trees</i> (OBST), which simply categorizes each element of a population into two binary classes.</p>
			</p>

			<div id="feature_space_split"></div>	
			<!--FIG 1: Scheme of an random forest objective-->

			<p>
			The model attributes a class to each member of the population via several questions (each based on a single feature) in the form of "Is feature $x_i$ larger than 0.5?". Depending on the answer (either yes or no) the population will be separated into two sub groups recursively until a criteria is met and the algorithm is stopped. The decision tree model can be visualized as a sequence of splits in the feature space, generating regions based on a set of hyperplanes (each inequality $x<\alpha$ represents an equation for a hyperplane that is normal to the $x_i$ axis of the feaure space). This can be visualized as follows (hover over the tree nodes to see its effect in the clasification):
			</p>

			<div id="decision_tree"></div>	

			<p>
			The nomenclature used for decision tress is similar to binary trees: each step where the population is separated is called a <b>node</b>. Each decedent node is called either by <b>child node</b> or decedent node. The algorithm starts at the <b>root node</b> (where the entire population is present as no split was made yet), and then follows the node branches until a classification is finally made at the last node of each branch, which is called a <b>leaf node</b>.
			</p>

			<p>
				This is an overview of the OBST structure, lets now focus on the individual steps that compose the overall algorithm:
			</p>
			<ol>
				<li><b>Candidate questions</b>: At each node, the set of questions that can be asked has to be decided in beforehand. Each question represents a specific binary split into two descendant nodes. Each node $t$ is associated with a specific subset $X_t$ of the full training set $X$. The split at node $t$ separates the population $X_T$ into two subsets, $X_{t,y}$ and $X_{t,n}$, of elements that answered "yes" or "no" to the question at this node, respectively. For each node the following is true:
					$$X_{t,y} \bigcap X_{t,n} = \emptyset$$
					$$X_{t,y} \bigcup X_{t,n} = X_t$$
				</li>
				<li><b>Splitting criterion</b>: To select the best split at each node we must choose a criterion that defines an "optimal" split.</li>
				<li><b>Stop splitting criterion</b>: To control the growth of the tree a stopping condition must be set, otherwise the tree would grown indefinitely;</li>
				<li><b>Class assignment rule</b>: Finally we must set a rule to assign a class to each leaf of the tree.</li>
			</ol>
			<p>
				Lets now discuss each one of these steps in more detail.
			</p>

			 <h3 class='sub-chapter'>
				 Candidate Questions
			 </h3>
			 <p>
			 	For the binary classification trees the set of candidate questions are in the form "Is feature $x_k \leq \alpha$?". For each feature, each value of $\alpha$ represents a different split of the subset $X_t$. If $X_k$ is a numerical variable then the number of possible $\alpha$ values is infinite, which is inviable for practical applications. To address this issue most algorithms choose the set of thresholds $\alpha$ considering the halfway between all consecutive distinct values of the feature $x_k$ in the population $X_t$
			 </p>

			<div id="decision_splits"></div>	

			 <p>
			 This type of question leads to linear segmentation of the feature hyperspace. More complex questions can lead to different kinds of splits, such as "Is features $X_k^2 < 5$. A relativelly common variation is the obblique decision tree, which uses a series of Householder matrices to reflect the training set, as definided by D. C. Wickramarachchi <i>et al</i> in <a href='https://www.sciencedirect.com/science/article/abs/pii/S0167947315002856' target='_'>"HHCART: An oblique decision tree</a>".
			 </p>

			 <h3 class='sub-chapter'>
				 Split criterion
			 </h3>

			 <p>
			 Each split generates two descendant nodes, each with populations $X_{t,y}$ and $X_{t,n}$. To choose the "best" split between all candidate questions a metric must be set. This design decision is very task dependant as the applications of decision trees can be quite diverse (in some use cases one might want to separate the population based on the variance or the relative entropy between each group, for example). Here we will discuss some common definitions, but for a more exhaustive list please refer to "Classification and Regression Trees" by Leo Breiman <i>et al</i>.
			 </p>


			<h4>
				Entropy
			</h4>

			<p>
			As the overall objective of the model is to split the population into groups that contains elements of a single class. A reasonable metric would, somehow, measure how "pure" a population is. There is one quantity in information theory that happens to measure exactly this, know as <b>Mutual Information</b>, which it is based on <b>Shannon entropy</b> (while the term "entropy" was borrowed from physics, it doesn't have the same exact interpretation, although there are some conceptual similarities between then).
			</p>
			<!--<p>
			In physics entropy is related to the general disorder of a system and has many interesting properties both in classical and in quantum physics. If you are interested in a more deep understanding of this topic I've made a brief introduction to it bellow, but feel free to skip it if you just want to know the information theory approach.
			 </p>-->
			 <p>
			In information theory the motivation that led to the formulation of entropy was to create a quantity (called as entropy by Shannon) to measure how much information is created by a process. The amount of information in a message or processes can be quantified by the amount of <b>surprise</b> it generates (here "surprise" is the correct technical term for the thing, believe it or not). A message that brings no new information generates no surprise in the reader, thus generating no entropy.
			 </p>
			 <p>
			 Lets define a <b>surprise function</b> as $I(p)$, where $p$ is an event (of any kind). Let $I(P_n)$ be a shorthand notation for the amount of surprise generated when the state of the system is equal to $x=x_n$. The total entropy is thus given by the average information gained from each individual state, weighted by the probability $P_n$ of said event happening:
			 </p>
			 $$S = \sum_n p_n I(p_n)$$
			 <p>
			 	As pointed out by Shannon, to correctly measure information the function $I_n$ should have some desirable properties:
			 </p>
			 <ol>
				 <li>$I(p) \geq 0$: Information must be a positive quantity.</li>
				 <li>$I(1) = 0$: If a event is certain $p=1$, then no surprise can come from it.</li>
				 <li>$I(p)$ should decrease monotonically in $p$. Events of higher probability  yield lower surprises.</li>
				 <li>$I(p_1 p2) = I(p_1) + I(p_2)$ The information gained from two statistically independent events is the sum of the gain from each individual event.</li>
			 </ol>
			 <p>
					The only function to satisfy all conditions, specially condition 4, is the logarithm (for a more strict demonstration please see the original paper):
				$$I(p) = - K \ln(p),$$
				where $K$ is a positive constant. The minus comes from condition 1: as $p\leq 1$ then the constant must be negative to keep the whole function positive. The log can be in any base as changing it would only affect the constant $a$. These properties can be readily observed from a plot of $I(p)$:
			 </p>
			<div id='log_plot'></div>

			<p>
				Finally we have that the (Shannon) entropy can be written as:
			$$\boxed{S = -K\sum_n p_n \ln(p_n)}.$$
			 </p>
			 <p>
					In summary, Shannon entropy quantifies the amount of information in $P_n$ as it a measure of the average surprise $\langle - \ln(p_n) \rangle$ from the transition to state $x_n$. With this we can calculate precisely the quality of each split based on the entropy of each subgroup:
			 </p>

			 <div id="split_criterion"></div>

			 <p>
				In practical applications the probabilities are estimated via the observed frequency of points belonging to each class.
			 </p>


			<h4>
				Gini Impurity
			</h4>
			<p>
				Another metric that measures the statistical purity of a population is the Gini Impurity, which is can be better understood via an example (from my experience at least). Consider a population of $N$ classes, where the observed frequency of each class $i$ is $p_i$ (i.e. the percentage of elements belonging to each class). The Gini impurity is the chance of mislabeling a random element of the population based on the observed frequencies $p_i$. If a population is "pure" (only has 1 class), then the impurity is 0 as a random element cannot be mislabeled. If there is the same number of elements on each class (maximum impurity) then the impurity is high as there is a $(N-1)/N$ probability of incorrectly labeling an random element.
			</p>
			<p>
			Formally, the probability of mislabeling an element with label $i$ is $\sum_{k \neq i}p_k = 1 - p_i$. Thus we can write de Gini inpurity $I_G$ as
			$$I_G = \sum_{i=1}^N (probability\ of\ selecting\ label\ i) \times (probability\ of\ mislabel\ i)$$

			$$I_G = \sum_{i=1}^N p_i (1-p_i) = \sum_{i=1}^N p_i - \sum_{i=1}^N p_i^2 = 1 - \sum_{i=1}^N p_i^2$$
			</p>
			<p>
			The Gini impurity has an interesting correlation with Shannon entropy as both of them are special cases of the <b>Tsallis entropy</b>
			$$S_q(p_i) = \frac{k}{q-1} \langle 1 - \sum_i p_i^q \rangle),$$
			where the Shannon entropy can be obtained with $q=1$ and the Giny Inpurity with $q=2$. Although the actual physical meaning of the Tsallis Entropy is an intensely (to put it gently) debated topic between physicists (as it is supposedly an entropy that is not addictive, which sounds like heresy to me), this relation between both measures of impurity is very cool nevertheless.
			</p>

			<h4>
				Variance Reduction
			</h4>
			<p>
				The metrics discussed thus far are well suited for classification tasks. If we wanted to use these metrics for a regression, the target variables should be transformed into categories so we could still calculate all the quantities discussed thus far. However this methodology can have a negative impact on the generalization capabilities of the model. One metric that is uniquely suited for regression tasks is the Variance Reduction $I_V(N)$, which consists of the difference between the variance in the parent node and its children:
				$$I_V(N) = Var(Y) - (Y_t) + Var(Y_n),$$
				where $Y$, $Y_t$, $Y_n$ are the set of target variables of the parent, left child and right child nodes and
				$$var(Y) \equiv  \frac{1}{N} \sum_{i=1}^N \sum_{j=1}^N \frac{1}{2} (y_i - y_j)$$
			</p>

			<h3>
				 Stop Splitting Rule
			</h3>
			<p>
				As the tree will recursively perform splits (based on the metrics discussed in the previous chapter), it needs to stop at some time. A sensible choice is to set a threshold on the split metric: if the maximum value of $\Delta I(t)$ exceeds the threshold, then the iteration is stopped. Although this is the most common rule used in practical situations, there are other options such as stop splitting when the cardinality of the subset $X_t$ is small or stop only when each subnode is "pure" (contains only elements of a single class).
			</p>

			<h3>
				 Class Assignment Rule
			 </h3>
			 <p>
			 This is perhaps the easiest step as to assign a class to a given node we can just use the class which is more common in that node. More formally the elements of a node will be assigned to class $i$ if $$arg\ max_i P(\omega_i|t).$$
			 </p>
			 <p>
			 For regression tasks the target variable assumes its mean value over the elements in the leaf node. These are but the simplest ways of class assignment rules, but alternatives such as using weighted averages are certainly viable.
			 </p>

			<h3>
				 Full Algorithm
			 </h3>
			 <p>
				 We are now in position to build an outline of the full binary classification tree algorithm. It can be written as follows:
			 </p>
			 <ol>
				 <li>
					 Start: Begin with the full population at the root node.
				 </li>
				 <li>
					 For each node t:
					 <ol>
						 <li>For each feature $x_k$:
							 <ol> 
								 <li>
									 For each candidate value $\alpha_{k,n}$, with $n=1,2,3,4,\dots, N_{t,k}$:
									 <ol>
										 <li>
											 Ask "Is $x_k \leq \alpha_{kn}$" and split the population into the subgroups $X_{t,y}$ and $X_{t,n}$ for the elements corresponding to the positive and negative outcome of the split question.
										 </li>
										 <li>
											 Compute the impurity decrease (there are several options for this metric).
										 </li>
									 </ol>
								 </li>
								 <li>
									 End
								 </li>
							 </ol>
						 </li>
						 <li>
							 Choose $x_{k*}$ (and associated $\alpha_{k*,n*}$) according to the split that lead to the overall maximum decrease in impurity.
						 </li>
						 <li>
							 If the stop splitting criterion is met, set the current node as a leaf and apply the class assignment rule to set the class label of the elements of $X_t$.
						 </li>
						 <li>
							 Else, generate two child nodes with associated subsets $X_{t,y}$ and $X_{t,n}$ depending to the outcome of the question $X_{k*} \leq \alpha_{k*\,n*}$
						 </li>
					 </ol>
				 </li>
			 </ol>


			 <h3>
				 Limmitations
			 </h3>

			 <p>
				 Decision trees pay a heavy price for their structural simplicity in their poor performance when compared to other, more complex, alternatives such as fancy regressions or neural networks. They are also very susceptible to overfitting as the only way for them to grasp the details of the data is by growing exponentially larger trees, which inevitably become extremely biased to the training set.
			 </p>
			 <p>
			 It is thus very important to grow sufficiently large trees so that its performance is acceptable but not too large so the tree is unable to extrapolate to new data. This challenge is similar to building large multilayer perception networks and similar techniques such as pruning have been applied to both cases. <!-- add a reference here -->
			 </p>
			<a name="Random Forests">
      </a>
      <h2 class="section">
       2. Random Forests
      </h2>


			<p>
			Random forests are a set of models from a larger family called <b>ensemble models</b>. These algorithms mix several "weak" learners to boost their overall quality, usually yielding impressive results. The random forest is an ensemble model that uses several decision trees as base learners (a forest of models). The resulting ensemble can potentially vastly outperform each individual regressor in many practical scenarios (we will discuss its limitations shortly). 
			</p>
			<p>
			This performance gain is obtained due to the combination of several intricate techniques, not by simply strapping decision trees together and hopping for the best. It was Leo Breiman that combined the ideas of bagging (bootstrap aggregation), stochastic learning and subspace selection to create the algorithm for random forests. He also introduced some novel insights such as estimating the generalization error as the out-of-bag (OOB) error and estimating the importance of each individual feature via permutation. We will now delve into each one of these ingredients and see how they help the random forest algorithm to outperform single Classification And Regression Trees (CARTs).
			</p>
			<h3 class='sub-chapter'>
				 Stochastic Discrimination
			</h3>
			<p>
				One major shortcoming of decision trees is their susceptibility to overfitting. Trees are, by themselves, not very good in terms of precision and need to grow in complexity to increase their performance. This need for complexity can easily lead to overfitting and poor generalization. Kleinbergs theory of stochastic discrimination (SD) showed that the combination of weak learners could not only be arbitrarily precise, but also fast, stable and resistant to overfitting.
			</p>
			<p>
				The stochastic discrimination process is defined as follows. Consider the two class problem with an initial population of $N$ elements. We also have an set of $S$ possible solutions, in our case each member of S is an week learner (weak here means just that the algorithm can be just slightly better than random guessing). We can think of the sequence of solutions as items appearing over time, and see the transitions of a given element between solutions as an stochastic process. It can be shown that for large numbers of solutions the average of the stochastic processes tends to two distinct limits, thus separations the dataset into two classes. This idea is represented in the following scheme which is a direct adaptation from the original scheme in Kleinbergs article:
			</p>
			<object class='center' type="image/svg+xml" data="stochastic_learning.svg">
					<!-- Your fall back here -->
					<img src="stochastic_learning.svg" />
			</object>

			<p>
				The fact that the SD process not only converges stably (on average over large sets of possible solutions) but also in polinomial time is a very powerful and intriguing result from SD theory. These demonstrations are far outside the scope of this article as they deserve a full article by themselves, but can be seen in Kleinberg original work, while the resistance to overfitting is directly addressed in a separate article "On the Algorithmic Implementation of Stochastic Discrimination" [Kleinberg 2000].
			</p>

			<h3>
				 Bagging
			</h3>
			<p>
				One way to improve the overall quality of a model is to use <b>bagging</b>, which simply put consists in training several instances of the same model, each over a different subset of the full training dataset. These models can be trained in parallel, which increases the overall speed substantially. The trained models can then be combined to form a single prediction (via a majority vote, for example). This scheme can be represented as follows.
			</p>
			<object type="image/svg+xml" data="bagging.svg">
					<!-- Your fall back here -->
					<img src="bagging.svg" />
			</object>
			<p>
				Consider a training set $X$ and associated targets $Y$. Bagging would consist of sampling with replacement both $X$ and $Y$ $n$ times, generating subsets $X_i$, $Y_i$, $i=1,2,\dots,n$. A model is then fitted on each subset $i$, resulting in a collection (or a bag) of models. The final prediction would then be the average over all $n$ models or by the majority vote.
			</p>

			<p>
			This method reduces the overall variance of the week base learners and also makes the model more resistant to noises in the training set. However, if one cannot generate many statistically independent trees (due to a lack of training points, for example), not much would be achieved from using this technique as all models would be almost identical and the variance would not be reduced.
			</p>
			<p>
			Random forests, however, do not only employ bagging to the training set, but also for the features themselves. Each model will be presented with only some of the features available, which leads to decision trees that are uncorrelated. This idea was initially presented by Hu to formulate Decision Forests (not to be confused with random forests), which is an algorithm that only employs feature bagging.
			</p>
			<p>
				The major advantage of hiding some features from the models is that if there is a feature that has a very strong correlation to the target variable, it would heavily influence all trees, making then all almost identical and heavily reliant on a single feature. This would lead to all trees being strongly correlated and reducing the overall performance of the bagging technique.
			</p>
			<p>
			Random forests as formulated by Breiman use both types of bagging. Each tree is trained not only in a subspace of the full feature hyperspace, but also presented with only a fraction of all observations available in the training set.
			</p>

			<h3>
				 Feature Importance
			</h3>
			<p>
				In many real world situations the overall lack of interpretability of random forests represents a major limitation as huge amounts of money or even lives might depend on the decisions of the model. In these situations knowing exactly what the model is doing is invaluable.
			</p>
			<p>
			Although there is no perfect way of interpreting a highly non-linear model such as random forests (models that can not be fully explained are also know as black-box models), there are ways of studying the importance of each feature on each prediction, and evaluate if the trained algorithm is considering sensible features for its decisions (like using age for predicting heart decease probability over eye color, which is an nigh useless feature for this task). We will discuss some of the most common ways of estimating the variable importance.
			</p>

			<h4>
				Permutation Feature Importance
			</h4>
			<p>
				Breiman defined a way of measuring the importance of each feature to the overall forest decisions. It consists of randomly shuffling (or permutating) the values of a single feature over the entire population and measuring how the prediction quality is affected.
			</p>
			<p>
				This method thus breaks the relationship between the variable and the target, and observes the subsequent drop in the models quality (in terms of a chosen quality metrics). This methodology is not restricted to only random forests and can be used in any model (its model agnostic).
			</p>
			<p>
			One shortcoming of this procedure is that it can be affected by multicolinearity. Suppose we have two very important and also very correlated features in the model. If one of these features is shuffled the model would still perform well as the other variable is still present. This would affect both variables almost identically, leading to an extremely low feature importance for two important features. See <a href='https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance_multicollinear.html#sphx-glr-auto-examples-inspection-plot-permutation-importance-multicollinear-py' target='_'>this sklean post about it</a>
			</p>

			<h4>
				Mean Decrease in Impurity
			</h4>
			<p>
					Another common way of measuring the importance of a feature is by directly observing the Mean Decrease in Impurity (MDI) in the nodes using this particular feature in all trees in the forest. This procedure is quite intuitive and straightforward, but it has two major limitations that must be taken into consideration.
			</p>
			<p>
				Firstly MDI feature importance has a bias towards high cardinality features (such as numerical features). Low cardinality features such as binary or categorical features would have their relative importances underestimated leading to a biased comprehension of the model. The second limitation is that this method only tells which variable was important during training, which some times is not equivalent to the importance of the features in the test set (during the prediction), as there is no way of measuring the decrease in impurity in such cases.
			</p>

			<form id="form" class="btn-group btn-group-toggle" data-toggle="buttons">
				<label  id='vimp_tree' class="btn btn-secondary active">
					<input type="radio" name="structeOption" value="MDI" checked>Show node importance<br>
				</label>
				<label  id='vimp_nodes' class="btn btn-secondary">
					<input type="radio" name="structeOption" value="TREE">Show average importance<br>
				</label>
			</form>

			<div id='feature_importance'></div>

			<h4>
				Average minimal depth
			</h4>
			<p>
				This is less know metric, but can be useful sometimes. The idea here is to evaluate the importance of a feature by its average minimal depth in the random forest (i.e. the average depth of the first appearance of this feature on each tree of the forest). The depth of a split is relevant as the first splits in a tree are usually the most important ones, as they split larger portions of the population and hugely affect the quality of the model (if the first split is bad the whole tree suffers, if the last split in a branch of a tree of depth 100 is bad then only this small subset is affected). In the following example a random forest of 3 small trees is illustrated, and the first occurrence of a feature is highlighted in each tree. In this example, the AMD of this feature would be $(1 + 1 + 2) / 3 = 1.33$. The other features always appears at depth 0 so its AMD is 0 (thus its the most important feature)
			</p>

			<div id='average_minimal_depth'></div>

			<p>
			A very modern variation to the Minimal Depth feature importance is the Surrugate Minimal Depth. This method consist of estimating the minimal depth via the first appearance not only to primary split variables but also to the occurrence of surrogate variables. This methodology is said to lead to better empirical estimation of the feature importance than the normal minimal depth approach <a href="(https://academic.oup.com/bioinformatics/article/35/19/3663/5368013" target='_'>Surrogate minimal depth as an importance measure for variables in random forests</a>
			</p>
			<p>
			As a closing note to this section, its worth porting out that the feature importance metrics are related to the model, not the features themselves (i.e. the importance is not of the feature itself, but to its influence on your model). <b>A bad feature can be very important to a bad model</b>.
			</p>

			<!--
			<h3>
				Relation to k-nearest neighbor algorithm
			</h3>
			<p>
			There is an interesting relationship between random forests and the KNN algorithm pointed by Lin and Jeon in "". The authors showed that both algorithms have a weighted neighborhood scheme. Let X be the features available and y be the corresponding target, a weighted neighborhood model aims to make predictions $\hat{y}$ for new points $x'$ by looking into its nearest neighbors, given by a weight function W:
			$$\hat{y} = \sum_{i=1}^n W(x_i, x') y_i$$
			where the weight function $W(x_i, x')$ is non-negative and represents the weight of the  i-th training point relative to $x'$. In the KNN the weight function is simply given by $W(x_i, x') = 1/k)$ if $x_i$ is one of the $k$ nearest neighbors (in some predefined metric system) closest to $x'$, and 0 otherwise.
			</p>
			<p>
			In decision trees the weights are also of the form $W(x_i, x')=(1/k)$, but $k$ here represents the points that are in the same leaf than $x'$. As a random forests combines the predictions of many decision trees, the resulting weight function of a random forest will be just the average of the weights of each of the trees:
			$$\hat{y} = \frac{1}{m} \sum+{j=1}^m \sum_{i=1}^n W_j (x_i, x') y_i$$
			where $m$ is the number of trees in the forest. Of course the neighborhood structure in random forest will be structurally very complex (as it adapts from the complexities of the training set). Its shown by the authors that the local shape of the neighborhood function adapts to the local importance of each feature (giving increased weights for important features locally)
		</p>
			-->


			<h3>
				Limitations
			</h3>
			<p>
				As discussed extensively, the major shortcomming of the random forest algorithm is the lack of interpretability. This limitation can be crippling when there is a considerable risk associated with the usage of the model, and decisions cannot be made completely blindly.
			</p>
			<p>
			There are many ways of trying to recover some (or all) interpretability of the RF model. The methods to extract he individual importance of each feature are powerful, but have some possible limitations that must be considered in beforehand. These feature importance metrics also <b>only represents estimates of the importance of the features</b> and can be biased in many cases.
			</p>
			<p>
			There are some modern techniques such as SHAP or LIME that try to approximate the final decision hyperplanes with locally interpretable regressors. These methods, while useful, also have limitations and possible biases. Its also proved that SHAP analysis a can be arbitrarily biased without affecting the performance of the model (this can be invisible to common validation techniques and may open the door for exploits in the model) as shown in <a href="https://arxiv.org/pdf/2002.11097.pdf" target='_'>Problems with Shapley-value-based explanations as feature importancemeasures</a>.
			</p>
			<p>
			The Born Again [T. Vidal and M. Schiffer] algorithm is very recent development and is a really interesting alternative to approach this problem. It offers a seemingly perfect solution: to summarize the entire random forest in a single decision tree that is as complex as the most complex tree in the forest, This solution represents an NP-hard problem and may not converge in a sensible time in some cases, so its a nice tool but it does not solve all problems.
			</p>
			<p>
			Another limitation of random forests is that the presence of multiple categorical variables the resulting ensemble may not perform better than a the base learner as show by Piryonesi <i>et al</i> in "The Application of Data Analytics to Asset Management: Deterioration and Climate Change Adaptation in Ontario Roads"
			</p>

			<a name="CODE">
      </a>
      <h2 class="section">
       3. Applications
      </h2>
      <p>
       Code:
      </p>
<pre>
{% include 'sketch.js' %}
</pre>

    </div>
   </div>
  </div>
  <hr/>
	<!-- mathjax -->
  <script async="" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>
	<!--load d3 images-->
	<script src="d3_charts.js">
	</script>
  <script src="../../js/script.js"> <!--TARGET-->
  </script>
  <!-- Bootstrap + JQuery -->
  <script crossorigin="anonymous" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" src="https://code.jquery.com/jquery-3.3.1.slim.min.js">
  </script>
  <script crossorigin="anonymous" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js">
  </script>
  <script crossorigin="anonymous" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js">
  </script>
  <script crossorigin="anonymous" src="https://kit.fontawesome.com/9ead9d8df4.js">
  </script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6">
  </script>
  <script>
   MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        svg: {
          fontCache: 'global'
        }
      };
  </script>
 </body>
</html>

