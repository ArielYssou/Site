<!DOCTYPE html>
<html lang="en">
 <head>
  <meta charset="utf-8"/>
  <meta content="width=device-width, intial-scale=1" name="viewport"/>
  <!-- Name on tab -->
  <title>
   Random Forests
  </title>
  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Cormorant+Garamond:400,700&display=swap" rel="stylesheet"/>
  <!-- Custom CSS -->
  <link crossorigin="anonymous" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" rel="stylesheet"/>
  <link href="../../css/style.css" rel="stylesheet"/> <!--TARGET-->
  <link href="../../css/pygments.css" rel="stylesheet"/> <!--TARGET-->
  <script src="https://cdn.jsdelivr.net/npm/p5@0.10.2/lib/p5.js">
  </script>
  <!-- Custom p5js animations. Add any other .js here -->
  <script src="sketch.js">
  </script>
  <script src="tree.js">
  </script>
 </head>
 <body>
  <!-- Navigation bar -->
  <nav class="navbar navbar-expand-lg navbar-dark fixed-top navbar-custom">
   <a class="navbar-brand" href="../.." style="font-weight:bold;"> <!--TARGET-->
    Ariel Yssou
   </a>
   <button aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation" class="navbar-toggler" data-target="#navbarSupportedContent" data-toggle="collapse" type="button">
    <span class="navbar-toggler-icon">
    </span>
   </button>
   <div class="collapse navbar-collapse" id="navbarSupportedContent">
    <ul class="navbar-nav mr-auto">
     <li class="nav-item">
      <a class="nav-link" href="../.." style="font-weight:bold;"> <!--TARGET--> 
       Home
      </a>
     </li>
     <li class="nav-item nav-blog active">
      <a class="nav-link" href="../../blog" style="font-weight:bold;"> <!--TARGET--> 
       Blog
       <span class="sr-only">
        (current)
       </span>
      </a>
     </li>
     <li class="nav-item">
      <a class="nav-link" href="../../gallery" style="font-weight:bold;"> <!--TARGET--> 
       Gallery
      </a>
     </li>
     <li class="nav-item">
      <a class="nav-link" href="../../about" style="font-weight:bold;"> <!--TARGET--> 
       About
      </a>
     </li>
    </ul>
   </div>
   <div class="collapse navbar-collapse" id="navbarSupportedContent">
    <ul class="navbar-nav ml-auto">
     <li class="nav-item">
      <a class="nav-link text-nowrap" href="https://github.com/ArielYssou" target="_blank">
       <i class="fab fa-github fa-lg">
       </i>
      </a>
     </li>
     <li class="nav-item">
      <a class="nav-link text-nowrap" href="https://www.linkedin.com/in/ariel-yssou-oliveira-f-2a07a5b5/" target="_blank">
       <i class="fab fa-linkedin fa-lg">
       </i>
      </a>
     </li>
    </ul>
   </div>
  </nav>
  <!-- End of navigation bar -->
  <div class="head_container">
   <img class="head_image no_blur" src="./cover_image.png"/>
   <div class="head_middle">
    <div class="head_text">
     <b>
      Random Forests
     </b>
    </div>
   </div>
  </div>
  <div class="container post_body">
   <div class="row post_struct">
    <div class="col-ld-12">
     <p>
      INTRO TEXT OF
      <b>
       Random Forests
      </b>
      .
     </p>
     <!-- TOC -->
     <table class="TOC">
      <tr>
       <th>
        Contents
       </th>
      </tr>
      <tr>
       <td>
        <a href="#INTRO">
         1. Introduction
        </a>
       </td>
      </tr>
      <tr>
       <td>
        <a href="#DECISION TREES">
         2. Decision trees
        </a>
       </td>
      </tr>
      <tr>
       <td>
        <a href="#Random Forests">
         2. Random Forests
        </a>
       </td>
      </tr>
      <tr>
       <td>
        <a href="#CODE">
         3. Applications
        </a>
       </td>
      </tr>
     </table>
     <br/>
     <a name="#INTRO">
     </a>
     <h2 class="section">
      1. Introduction
     </h2>

		 Random forest are one of the most famous and common models in Machine learning as they are easy to implement, very powerful, adaptable and are fast to train. Most texts about these models are focused on the practical implementation of random forests but usually lack a deeper view into the statistics that provide such excellent results. This post will be focused on providing a more solid view into the theory of these models and study why they are so powerful and their potential shortcomings.

		 The text is structured as follows. Firstly we will discus <b>Decision Trees</b>, which are the base models that compose a random forests. With this formalism we will be able to discuss the theory of Random forests in chapter 2. The last chapter will be dedicated to the implementation of RFs in python using the XXXX dataset.

			<a name="DECISION TREES">
      </a>
      <h2 class="section">
       2. Decision trees
      </h2>
			A decision tree is a predictive modeling technique that make predictions based on a hierarchy of decisions, which can be used both for classification or regression purposes. The simplest and most common type of decision tree is the <i>Ordinary binary classification trees</i> (OBST), which  has the objective of attributing a class label to each input  based on a set binary questions (yes or no questions). 

			To illustrate the overall architecture of a decision tree, lets consider the task of classifying an population on two Lets consider an example illustrating a decision tree architecture on the classification of a population can be represented as follows:

			<!-- Image 1: Separando uma população em duas classes com perguntas -->

			The tree is then consists of a set of <b>nodes</b> that split the population. The algorithm starts at the root node where the entire population is splitted, and then follows the node branches until a classification is made at the last node, called a "leaf".

			<!-- Scheme of a tree -->
			

			Decision trees are one of the simplest models available as they consist of a simple hierarchy of questions. Albeit they usually wield lest than ideal results (in terms of precision) they are still widely used as their simple formulation allows for the complete interpretation of the model. This characteristics can be very desirable as in some practical cases when there is a high risk associated with the usage of the model and the pratictioner cannot risk everything in a black box model.

			<p>The most common type of decision trees are , which split the feature space via a sequence of decisions, each base on a single feature and are in the form of "is feature $i$ greater that 1/2?". Depending on the answer (either yes or no). The population will be splitted until a criteria is met and the algorithm is stopped. In the end the population will be separated in different groups, each on a leaf of the tree. I the objective of the tree is a classification the tree will atributed a class to each leaf and if the objective is regression then the target variable will be estimated as the mean of the population of each leaf.</p>

			TO further increase our undertanding of decision trees lets consider the following scenario. Say that we have a mixed population of Hobbits (class $\omega_1$) and Dwarfs 9class $\omega_2$) From each individual we know only 3 features: height, weight and age. From the lore, Hobbit heights range from 0.9 to 1.2 meters while dawarfs are 1.2 to 1.5m tall.

			The basic idea behind OBCT is to sequentially split the feature space into regions corresponding to different classes, as represented in the following animation:

			<!-- Example of a decision tree -->

			the thask illustrated in the example above is in a two dimensional space wich has clear geometric splits. This, however, is usually not the case in practical cases where the feature space is very high dimensional and a geometric approach cannot be achieved. In order to generalize the algorithm, we need to establish the following elements in the training phase:
			<ol>
				<li>Candidate questions: At each node the set of questions that can be asked has to decided. Each question represents a specific binary split into two descendant nodes. Each node $t$ is associated with a specific subset $X_t$ of the full training set $X$. The split at node $t$ separates the population $X_T$ into two subsets , $X_{t,+}$ and $X_{t,-}$, that correspond to the positive and negative answer to the question (yes and no respectively). Thus the following is true:
					$$X_{t,+} \bigcap X_{t,-} = \emptyset$$
					$$X_{t,+} \bigcup X_{t,-} = X_t$$
				</li>
				<li>Splitting criterion: To select the best split at each node we must choose a criterion.</li>
				<li>Stop splitting criterion: To control the growth of the tree a stopping condition must be set, otherwise the tree would grown indefinitely;</li>
				<li>Class assignment rule: We must set a rule to assign a class to each leaf</li>
			</ol>
			Now we are now read to delve into each of the elements above.
			 <h3>
				 Candidate Questions
			 </h3>
			 For the binary classification type of trees, the quests are in the form "Is feature $x_k$ \leq \alpha$?". For each feature, each value of $alpha$ represents a different split of the subset $X_t$. If $X_k$ is a numerical variable, then the number of possible $\alpha$ values is infinite. In practice we can only consider a finite set of questions. Most algorithms choose the set of thresholds $\alpha$ considering the halfway between all consecrates distinct values of $x_k$ in the population $X_t$
			 <h3>
				 Split criterion
			 </h3>
			 Each split generates two descendant nodes, each with populations $X_{t,+}$ and ${X_{t,-}$. But how can we determine which split is "best"? It depends on what we want to achieve with the algorithm. It makes sense to split the population into groups that are more "class homogenous" than the ancestor subset $X_t$. the goal is then to define a metric that quantifies impurity or homogeneity.
			<h4>
				Entropy
			</h4>
			 In information theory there is the notion of impurity which is based on Shannon entropy. While the term "entropy" was borrowed from physics, it doesn't have the same exact interpretation, although there are some conceptual semblances between then.
			 In physics entropy is related to the general disorder of a system, It has many interesting properties both in classical and in quantum physics. If you are interested in a more deep understanding of this topic I've made a brief introduction to it bellow, but feel free to skip it if you just want to know the information theory approach.
			 In information theory entropy has a different meaning. The idea is to create a quantity (called as entropy by Shannon) to measure how much information is created by a process. The amount of information in a message or processes can be quantified by the amount of <b>surprise</b> it generates (here surprise is the correct technical term for the thing, believe it or not). A message that brings no new information generates no surprise in the reader, thus generating no entropy.
			 Lets define a <b>surprise function</b> as $I(p)$, where $p$ is an event (thus $I(P_n)$ is the amount of surprise generated when the state of the system is equal to $x=x_n$. The total entropy is thus given by the average  information gained from each individual state weighted by the probability $P_n$ of said event happening:
			 $$S = \sum_n p_n I(p_n)$$
			 to correctly measure information, the function$I_n$ should have some desirable properties (as stated by Shannon in his original study):
			 <ol>
				 <li>$I(p) \geq 0$ Information must be a positive quantity.</li>
				 <li>$I(1) = 0$: If a event is certain $p=1$, then no surprise can come from it.</li>
				 <li>$I(p)$ should decrease monotonically in $p$. Events of higher probability  yield lower surprises.</li>
				 <li>$I(p_1 p2) = I(p_1) + I(p_2)$ The information gained from two statistically independent events is the sum of the gain from each individual event.</li>
			 </ol>
			The only function to satisfy all conditions, specially condition 4, is the logarithm (for a more thought demonstration see the original paper):
			$$I(p) = - K \ln(p),$$
			where $K$ is a positive constant. The minus comes from condition 1, as $p\leq 1$ then the constant must be negative to keep the whole function positive. The log can be in any base as changing it would only affect the constant $a$. Finally we have that the (Shannon) entropy can be written as:
			$$\boxed{S = -K\sum p_n \ln(p_n)}.$$
			In summary, the entropy in information theory quantifies the amount of information in $P_n$ as it a measure of the average surprise $\langle - \ln(p_n) \rangle$ from the transition to state $x_n$.
			Thus a decision forest can split a population based on the total entropy of each subset. In practice probabilities are estimated via the observed frequency of points belonging to each class.
			<h4>
				Gini Impurity
			</h4>
			Lets consider a population of $N$ classes, where the observed frenquency of each class $i$ (i.e. the percentage of elements belonging to each class) is $pa_i$. The Gini impurity is the chance of mislabeling a random element of the population based on the observed frequencies $p_i$. If a population is "pure" (only has 1 class), then the impurity is 0 as it cannot be mislabeled. If there is the same number of elements on each class then the impurity is high as there is a $(N-1)/N$ probability of incorrectly labeling an element.
			Formally the probability of mislabeling an element with label $i$ is $\sum_{k \neq i}p_k = 1 - p_i. Thus we can write de Gini inpurity $I_G$ as
			$$I_G = \sum_{i=1}^N (probability\ of\ selecting\ label\ i) \times (probability\ of\ mislabel\ i)$$
			$$I_G = \sum_{i=1}^N p_i (1-p_i) = \sum_{i=1}^N p_i - \sum_{i=1}^N p_i^2 = 1 - \sum_{i=1}^N p_i^2
			The Gini impurity has a interesting correlation with the Shannon entropy as both of them are special cases of the Tsallis entropy
			$$S_q(p_i) = \frac{k}{q-1} \langle 1 - \sum_i p_i^q \rangle),$$
			where the Shannon entropy can be obtained with $q=1$ and the Giny Inpurity with $q=2$. Although the actual physical meaning of the Tsallis Entropy is debatable (as it is supposedly an entropy that is not addictive), this relation between both measures of impurity is very cool nevertheless.

			<h4>
				Measure of "goodness"
			</h4>


			<h4>
				Variance Reduction
			</h4>
			In regression tasks, we don't seek to atribute labels to elements, but continuous values (regression trees), thus our previous metrics can't be directly applied (not without performing a discretization of the variables, which may be inappropriate for regression purposes). One such metric is Variance Reduction $I_V(N)$, which consists of the difference between the variance in the parent node and its childs:
			$$I_V(N) = Var(Y) - (Y_t) + Var(Y_f),$$
			where Y, Y_t, Y_f are the set of target variables of the parent, left child and right child nodes and
			$$var(Y) \equiv  \frac{1}{N} \sum_{i=1}^N \sum_{j=1]^N \frac{1}{2} (y_i - y_j)$$

			<h3>
				 Stop Splitting Rule
			</h3>
			As the tree will recursively realize splits based on the metrics discussed, it needs to stop at some time. A sensible choice is to set a threshold on the split metric. If the maximum value of $\Delta I(t)$ exceed the threshold, then the iteration is stopped.

			<h3>
				 Class Assignment Rule
			 </h3>
			<h3>
				 Full Algorithm
			 </h3>
			<a name="Random Forests">
      </a>
      <h2 class="section">
       2. Random Forests
      </h2>

			Main text

			<a name="CODE">
      </a>
      <h2 class="section">
       3. Applications
      </h2>
      <p>
       Code section
      </p>
       <span class="alert">
        Click on the panel bellow to start/stop the simulation.
       </span>
       (In some browsers is necessary to click twice to unpause because I screwed the html in some unknown way)
      </p>
      <div class="sketch-holder" id="c1">
      </div>
      <p>
       Other canvas
      </p>
      <div class="sketch-holder" id="c2">
      </div>

      <p>
       Code:
      </p>
<pre>
{% include 'sketch.js' %}
</pre>

    </div>
   </div>
  </div>
  <hr/>
  <script src="../../js/script.js"> <!--TARGET-->
  </script>
  <!-- Bootstrap + JQuery -->
  <script crossorigin="anonymous" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" src="https://code.jquery.com/jquery-3.3.1.slim.min.js">
  </script>
  <script crossorigin="anonymous" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js">
  </script>
  <script crossorigin="anonymous" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js">
  </script>
  <script crossorigin="anonymous" src="https://kit.fontawesome.com/9ead9d8df4.js">
  </script>
  <!-- mathjax -->
  <script async="" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6">
  </script>
  <script>
   MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        svg: {
          fontCache: 'global'
        }
      };
  </script>
 </body>
</html>

