<!DOCTYPE html>
<html lang="en">
 <head>
  <meta charset="utf-8"/>
  <meta content="width=device-width, intial-scale=1" name="viewport"/>
  <!-- Name on tab -->
  <title>
   Random Forests
  </title>
  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Cormorant+Garamond:400,700&display=swap" rel="stylesheet"/>
  <!-- Custom CSS -->
  <link crossorigin="anonymous" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" rel="stylesheet"/>
  <link href="../../css/style.css" rel="stylesheet"/> <!--TARGET-->
  <link href="../../css/pygments.css" rel="stylesheet"/> <!--TARGET-->
	<!-- Load p5.js -->
  <script src="https://cdn.jsdelivr.net/npm/p5@0.10.2/lib/p5.js">
  </script>
	<!-- Load d3.js -->
	<script src="https://d3js.org/d3.v4.js"></script>
  <!-- Custom p5js animations. Add any other .js here -->
  <script src="sketch.js">
  </script>
  <script src="tree.js">
  </script>

 </head>
 <body>
  <!-- Navigation bar -->
  <nav class="navbar navbar-expand-lg navbar-dark fixed-top navbar-custom">
   <a class="navbar-brand" href="../.." style="font-weight:bold;"> <!--TARGET-->
    Ariel Yssou
   </a>
   <button aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation" class="navbar-toggler" data-target="#navbarSupportedContent" data-toggle="collapse" type="button">
    <span class="navbar-toggler-icon">
    </span>
   </button>
   <div class="collapse navbar-collapse" id="navbarSupportedContent">
    <ul class="navbar-nav mr-auto">
     <li class="nav-item">
      <a class="nav-link" href="../.." style="font-weight:bold;"> <!--TARGET--> 
       Home
      </a>
     </li>
     <li class="nav-item nav-blog active">
      <a class="nav-link" href="../../blog" style="font-weight:bold;"> <!--TARGET--> 
       Blog
       <span class="sr-only">
        (current)
       </span>
      </a>
     </li>
     <li class="nav-item">
      <a class="nav-link" href="../../gallery" style="font-weight:bold;"> <!--TARGET--> 
       Gallery
      </a>
     </li>
     <li class="nav-item">
      <a class="nav-link" href="../../about" style="font-weight:bold;"> <!--TARGET--> 
       About
      </a>
     </li>
    </ul>
   </div>
   <div class="collapse navbar-collapse" id="navbarSupportedContent">
    <ul class="navbar-nav ml-auto">
     <li class="nav-item">
      <a class="nav-link text-nowrap" href="https://github.com/ArielYssou" target="_blank">
       <i class="fab fa-github fa-lg">
       </i>
      </a>
     </li>
     <li class="nav-item">
      <a class="nav-link text-nowrap" href="https://www.linkedin.com/in/ariel-yssou-oliveira-f-2a07a5b5/" target="_blank">
       <i class="fab fa-linkedin fa-lg">
       </i>
      </a>
     </li>
    </ul>
   </div>
  </nav>
  <!-- End of navigation bar -->
  <div class="head_container">
   <img class="head_image no_blur" src="./cover_image.png"/>
   <div class="head_middle">
    <div class="head_text">
     <b>
      Random Forests
     </b>
    </div>
   </div>
  </div>
  <div class="container post_body">
   <div class="row post_struct">
    <div class="col-ld-12">
     <p>
      INTRO TEXT OF
      <b>
       Random Forests
      </b>
      .
     </p>
     <!-- TOC -->
     <table class="TOC">
      <tr>
       <th>
        Contents
       </th>
      </tr>
      <tr>
       <td>
        <a href="#INTRO">
         1. Introduction
        </a>
       </td>
      </tr>
      <tr>
       <td>
        <a href="#DECISION TREES">
         2. Decision trees
        </a>
       </td>
      </tr>
      <tr>
       <td>
        <a href="#Random Forests">
         2. Random Forests
        </a>
       </td>
      </tr>
      <tr>
       <td>
        <a href="#CODE">
         3. Applications
        </a>
       </td>
      </tr>
     </table>
     <br/>
     <a name="#INTRO">
     </a>
     <h2 class="section">
      1. Introduction
     </h2>

		 <p>
		 	Random forest are one of the most famous and common models in Machine learning as they are easy to implement, adaptable, offer excellent performance and are fast to train. Most texts about these models are focused on the practical side random forests but usually skip over the underling theory. This post will be focused on the theoretical side of the model, aiming to shed some light into this model and study why it is so highly performant and reliable, also providing some insights into some potential shortcomings.
		 </p>

		 <p>
		 	The text is structured as follows. Firstly we will discus <b>Decision Trees</b>, which are the building blocks of random forests. With this formalism we will be able to discuss the proper theory of Random forests in Chapter 2. The last chapter will be dedicated to a simple and illustrative implementation of RFs in python using the XXXX dataset, with an emphasis on discussing how to extract the feature importance of a trained model.
		 </p>

			<a name="DECISION TREES">
      </a>
      <h2 class="section">
       2. Decision trees
      </h2>
			<p>Decision trees are a class of models whose predictions are based on a hierarchy of decisions, which can be used both for classification or regression purposes. Due to their structural simplicity, these models are fully interpretable. The simplest and most common type of decision tree is the <i>Ordinary binary classification trees</i> (OBST), which simply categorizes each element of a population into two binary classes.</p>

			<div id="feature_space_split"></div>	
			<!--FIG 1: Scheme of an random forest objective-->

			<p>
				The model attributes a class to each member of the population via several questions, each based on a single feature in the form of "Is feature $x_i$ larger than 0.5?". Depending on the answer (either yes or no) the population will be separated into two sub groups until a criteria is met and the algorithm is stopped. The decision tree model can be visualized as a model that splits the feature space into regions based on a set of hyperplanes (each inequality $x<\alpha$ represents the equation for an hyperplane that is normal to the $x_i$ axis of the feaure space):
			</p>

			<div id="decision_tree"></div>	

			<p>
				Each step that a question is made and the population is separated is called a <b>node</b>. The algorithm starts at the root node where the entire population is present, and then follows the node branches until a classification is finally made at the last node of each branch, which is called a <b>leaf node </b>.
			</p>

			<!-- Scheme of a tree -->
			

			<p>
				Now that the overall scheme of a OBST is laid out, lets focus on the individual tasks that compose the overall algorithm:
			</p>
			<ol>
				<li>Candidate questions: At each node the set of questions that can be asked has to decided. Each question represents a specific binary split into two descendant nodes. Each node $t$ is associated with a specific subset $X_t$ of the full training set $X$. The split at node $t$ separates the population $X_T$ into two subsets , $X_{t,y}$ and $X_{t,f}$, of elements that asserted "yes" or "no" to the question at this node, respectively. Thus the following is true:
					$$X_{t,+} \bigcap X_{t,-} = \emptyset$$
					$$X_{t,+} \bigcup X_{t,-} = X_t$$
				</li>
				<li>Splitting criterion: To select the best split at each node we must choose a criterion that defines an "optimal" split.</li>
				<li>Stop splitting criterion: To control the growth of the tree a stopping condition must be set, otherwise the tree would grown indefinitely;</li>
				<li>Class assignment rule: Finally we must set a rule to assign a class to each leaf of the tree.</li>
			</ol>
			<p>
				Lets now discuss each one of these mode design elements
			</p>

			 <h3>
				 Candidate Questions
			 </h3>
			 <p>
			 	For the binary classification type of trees the set of candidate questions made in the trees nodes are in the form "Is feature $x_k \leq \alpha$?". For each feature, each value of $\alpha$ represents a different split of the subset $X_t$. If $X_k$ is a numerical variable, then the number of possible $\alpha$ values is infinite. In practice we can only consider a finite set of questions. Most algorithms choose the set of thresholds $\alpha$ considering the halfway between all consecutive distinct values of $x_k$ in the population $X_t$
			 </p>
			 <p>
				 This type of question leads to linear segmentation of the feature hyperspace. More complex questions can lead to different kinds of splits, such as "Is features $X_k^2 < 5$. A relativelly common variery is the obblique decision trees which uses a series of Householder matrices to reflect the training set, as definided by D. C. Wickramarachchi <i>et al</i> in "HHCART: An oblique decision tree"https://www.sciencedirect.com/science/article/abs/pii/S0167947315002856.
			 </p>


			 <h3>
				 Split criterion
			 </h3>


			 <p>
			 	Each split generates two descendant nodes, each with populations $X_{t,y}$ and $X_{t,n}$. To choose the "best" split between all candidate questions a metric must be set. This design decision is very task dependant as the applications of decision trees can be quite diverse (in some applications one might want to separate the population based on the variance or even on the relative entropy between each group). Here we will discuss some common definitions, but for a more exhaustive list please refer to "".
			 </p>

			<h4>
				Entropy
			</h4>

			As the overall objective of the model is to split the population into groups that contain elements of a single class. A reasonable metric would thus, somehow, measure how "pure" a population is. There is one quantity in information theory that happens to measure exactly this know as <b>Mutual Information</b>, which it is based on <b>Shannon entropy</b> (while the term "entropy" was borrowed from physics, it doesn't have the same exact interpretation, although there are some conceptual similarities between then).
			 In physics entropy is related to the general disorder of a system and has many interesting properties both in classical and in quantum physics. If you are interested in a more deep understanding of this topic I've made a brief introduction to it bellow, but feel free to skip it if you just want to know the information theory approach.
			 In information theory entropy has a different meaning. The idea is to create a quantity (called as entropy by Shannon) to measure how much information is created by a process. The amount of information in a message or processes can be quantified by the amount of <b>surprise</b> it generates (here "surprise" is the correct technical term for the thing, believe it or not). A message that brings no new information generates no surprise in the reader, thus generating no entropy.
			 Lets define a <b>surprise function</b> as $I(p)$, where $p$ is an event (of any kind). Let $I(P_n)$ be a shorthand notation for the amount of surprise generated when the state of the system is equal to $x=x_n$. The total entropy is thus given by the average information gained from each individual state, weighted by the probability $P_n$ of said event happening:
			 $$S = \sum_n p_n I(p_n)$$
			 <p>
			 	As pointed out by Shannon, to correctly measure information the function$I_n$ should have some desirable properties:
			 </p>
			 <ol>
				 <li>$I(p) \geq 0$ Information must be a positive quantity.</li>
				 <li>$I(1) = 0$: If a event is certain $p=1$, then no surprise can come from it.</li>
				 <li>$I(p)$ should decrease monotonically in $p$. Events of higher probability  yield lower surprises.</li>
				 <li>$I(p_1 p2) = I(p_1) + I(p_2)$ The information gained from two statistically independent events is the sum of the gain from each individual event.</li>
			 </ol>
			 <p>
					The only function to satisfy all conditions, specially condition 4, is the logarithm (for a more strict demonstration please see the original paper):
				$$I(p) = - K \ln(p),$$
				where $K$ is a positive constant. The minus comes from condition 1, as $p\leq 1$ then the constant must be negative to keep the whole function positive. The log can be in any base as changing it would only affect the constant $a$. Finally we have that the (Shannon) entropy can be written as:
			$$\boxed{S = -K\sum p_n \ln(p_n)}.$$
			 </p>
			 <p>
					In summary, Shannon entropy quantifies the amount of information in $P_n$ as it a measure of the average surprise $\langle - \ln(p_n) \rangle$ from the transition to state $x_n$.
			 </p>
			 <p>
				With this discussion we can see that the entropy is a valid metric to be used as a basis for the split criterion. In practical applications the probabilities are estimated via the observed frequency of points belonging to each class.
			 </p>


			<h4>
				Gini Impurity
			</h4>
			<p>
				Another metric that measures the statistical purity of a population is the Gini Impurity, which is can be better understood via an example (from my experience at least). Consider a population of $N$ classes, where the observed frequency of each class $i$ (i.e. the percentage of elements belonging to each class) is $p_i$. The Gini impurity is the chance of mislabeling a random element of the population based on the observed frequencies $p_i$. If a population is "pure" (only has 1 class), then the impurity is 0 as it cannot be mislabeled. If there is the same number of elements on each class then the impurity is high as there is a $(N-1)/N$ probability of incorrectly labeling an element.
			</p>
			<p>
			Formally the probability of mislabeling an element with label $i$ is $\sum_{k \neq i}p_k = 1 - p_i$. Thus we can write de Gini inpurity $I_G$ as
			$$I_G = \sum_{i=1}^N (probability\ of\ selecting\ label\ i) \times (probability\ of\ mislabel\ i)$$

			$$I_G = \sum_{i=1}^N p_i (1-p_i) = \sum_{i=1}^N p_i - \sum_{i=1}^N p_i^2 = 1 - \sum_{i=1}^N p_i^2$$
			</p>
			<p>
			The Gini impurity has a interesting correlation with the Shannon entropy as both of them are special cases of the Tsallis entropy
			$$S_q(p_i) = \frac{k}{q-1} \langle 1 - \sum_i p_i^q \rangle),$$
			where the Shannon entropy can be obtained with $q=1$ and the Giny Inpurity with $q=2$. Although the actual physical meaning of the Tsallis Entropy is debatable (as it is supposedly an entropy that is not addictive), this relation between both measures of impurity is very cool nevertheless.
			</p>


			<h4>
				Variance Reduction
			</h4>
			<p>
				The metrics discussed thus far are well suited for classification tasks. For a regression, the target variables should be transformed into categories so we could still use these metrics, which can have a negative impact on the generalization capabilities of the model. One metric that is uniquely suited for regression tasks is the Variance Reduction $I_V(N)$, which consists of the difference between the variance in the parent node and its childs:
				$$I_V(N) = Var(Y) - (Y_t) + Var(Y_f),$$
				where $Y$, $Y_t$, $Y_f$ are the set of target variables of the parent, left child and right child nodes and
				$$var(Y) \equiv  \frac{1}{N} \sum_{i=1}^N \sum_{j=1}^N \frac{1}{2} (y_i - y_j)$$
			</p>

			<h3>
				 Stop Splitting Rule
			</h3>
			<p>
				As the tree will recursively realize splits based on the metrics discussed, it needs to stop at some time. A sensible choice is to set a threshold on the split metric: if the maximum value of $\Delta I(t)$ exceed the threshold, then the iteration is stopped. Although this is the most common rule used in practical situations, there are other options such as stop splitting when the cardinality of the subset $X_t$ is small or stop only when each subnode is "pure" (contains only elements of a single class).
			</p>

			<h3>
				 Class Assignment Rule
			 </h3>
			 <p>
			 Perhaps the easiest step, to assign a class to a given node we can just use the class which is more common in that node. More formally the elements of a node will be assigned to class $i$ if $$arg\ max_i P(\omega_i|t).$$. For regression tasks the target variable assumes its mean value over the elements in the leaf node. These are but the simplest ways of class assignment rules, but alternatives such as using weighted averages are certainly viable.
			 </p>

			<h3>
				 Full Algorithm
			 </h3>
			 <p>
				 We are now in position to build an outline of the full binary classification tree algorithm. It can be written as follows:
			 </p>
			 <ol>
				 <li>
					 Start: Build a root node and pass it the entire population
				 </li>
				 <li>
					 For each node t:
					 <ol>
						 <li>For each feature $x_k$:
							 <ol> 
								 <li>
									 For each candidate value $\alpha_{k,n}$, with $n=1,2,3,4,\dots, N_{t,k}$:
									 <ol>
										 <li>
											 Ask "Is $x_k \leq \alpha_{kn}$" and split the population into the subgroups $X_{t,y}$ and $X_{t,n}$ for the elemtens corresponding to the positive and negative outcome of the split question.
										 </li>
										 <li>
											 Compute the impurity decrease (there are several options for this metric)
										 </li>
									 </ol>
								 </li>
								 <li>
									 End
								 </li>
							 </ol>
						 </li>
						 <li>
							 Choose $x_{k*}$ (and associated $\alpha_{k*,n*}$) according to the split that lead to the overall maximum decrease in impurity.
						 </li>
						 <li>
							 If the stop splitting criterion is met, set the current node as a leaf and apply the class assignment rule to set the class label of the elements of $X_t$.
						 </li>
						 <li>
							 Else, generate two child nodes with associated subsets $X_{t,y}$ and $X_{t,n}$ depending to the outcome of the question $X_{k*} \leq \alpha_{k*\,n*}$
						 </li>
					 </ol>
				 </li>
			 </ol>


			 <h3>
				 Limmitations
			 </h3>

			 <p>
				 Decision trees pay a heavy price for their structural simplicity in their general poor performance when compared to other, more complex, alternatives such as fancy regressions or neural networks. They are also very susceptible to overfitting as the only way for them to grasp the details of the data is by growing exponentially larger trees, that are too biased by the training set.
			 </p>
			 </p>
			<a name="Random Forests">
      </a>
      <h2 class="section">
       2. Random Forests
      </h2>


			<p>
				Random forests are a set of models from a larger family called ensemble models. These kinds of algorithms mix several "weak" learners to boost their overall quality, usually yielding impressive gains. The random forest is an ensemble model that uses several decision trees as base learners (a forest if you will). The resulting ensemble can potentially vastly outperform each individual regressor in many practical scenarios (we will discuss its limitations shortly). This performance gain is due to the combination of several techniques, not by simply strapping decision trees together and hopping for the best. It was Leo Breiman that combined the ideas of bagging (bootstrap aggregation), stochastic learning and subspace selection to create the algorithm for random forests. He also introduced some novel insights such as estimating the generalization error as the out-of-bag (OOB) error and estimating the importance of each individual feature via permutation. We will now delve into each one of these ingredients and see how they help the random forest algorithm to outperform single Classification And Regression Trees (CARTs).
			</p>


			<h3>
				 Stochastic Learning
			</h3>
			<p>
				One major shortcoming of decision trees is their susceptibility to overfitting. Trees are, by themselves, not very good in terms of precision and need to grow in complexity to increase their performance. This need for complexity can easily lead to overfitting and poor generalization. Kleinbergs theory of stochastic discrimination (SD) showed that the combination of weak learners could not only be arbitrarily precise, but also fast, stable and resistant to overfitting.
			</p>
			<p>
				The stochastic discrimination process is defined as follows. Consider the two class problem with an initial population of $N$ elements. We also have an set of $S$ possible solutions, in our case each member of S is an week learner (weak here means just that the algorithm can be just slightly better than random guessing). We can think of the sequence of solutions as items appearing over time, and see the transitions of a given element between solutions as an stochastic process. It can be shown that for large numbers of solutions the average of the stochastic processes tend to two distinct limits, thus allowing for a more robust discrimination, as represented in the following scheme (an direct adaptation from the original scheme in Kleinbergs article):
			</p>
			<p>
				The fact that the SD process not only converges stably (on average over large sets of possible solutions) but also in polinomial time are very powerful and intriguing results from SD theory. These demonstrations are far outside the scope of this article as they deserve a full article by themselves, but can be seen in Kleinberg original work, while the resistance to overfitting is directly addressed in a separate article "".
			</p>

			<h3>
				 Bagging
			</h3>
			<p>
				One way to improve the overall quality of a model is to use <b>bagging</b>, which simply put consists in training several instances of the seme model, but training each over different subsets of the entire training set and then combine all the resulting fitted models to generate predictions.
			</p>
			<p>
				Consider a training set $X$ and associated targets $Y$. Bagging would consist of sampling with replacement both $X$ and $Y$ $n$ times, generating subsets $X_i$, $Y_i$, $i=1,2,\dots,n$. Then a model is fitted on each subset $i$. The final prediction would then be the average over all $n$ models or by the majority vote.
			</p>
			<p>
			This method reduces the overall variance of the week base learners and also makes the model more resistant to noises in the training set. However, if one cannot generate many statistically independent trees (due to a lack of training points, for example), not much would be achieved from using this technique as all models would be almost identical and the variance would not be reduced.
			</p>
			<p>
			Random forests, however, do not only employ bagging to the training set, but also for the features themselves. Each model will be presented with only some of the features available, which leads to decision trees that are uncorrelated. This idea was initially presented by Hu to formulate Decision Forests (not to be confused with random forests), which is an algorithm that only employs feature bagging.
			</p>
			<p>
				The major advantage of hiding some features from the models is that if there is a feature that has a very strong correlation to the target variable, it would heavily influence all trees, making then all almost identical and heavily reliant on a single feature. This would lead to all trees being strongly correlated and reducing the gains from bagging.
			</p>
			<p>
			Random forests as formulated bu Breiman use both types of bagging. Each tree is trained not only in a subspace of the full feature hyperspace, but also presented with only a fraction of all observations available in the training set.
			</p>


			<h3>
				 Feature Importance
			</h3>
			<p>
				In many real world situations the overall lack of interpretability of random forests represents a major limitation as huge amounts of money or even lives might depend on the decisions of the model. In these situations knowing exactly what the model is doing is invaluable.
			</p>
			<p>
			Although there is no perfect way of interpreting a highly non-linear model such as random forests, there are ways of studding the importance of each feature on each prediction, and evaluate if the trained algorithm is considering sensible features for its decisions (like using age as a important feature for predicting heart decease probability over eye color, which is a useless feature for this task). We will discuss some of the most common ways of estimating the variable importance.
			</p>


			<h4>
				Permutation Feature Importance
			</h4>
			<p>
				Breiman defined a way of measuring the importance of each feature to the overall forest decisions. It consists of randomly shuffling (or permutating) the values of a single feature over the entire population and measuring how the prediction quality is affected.
			</p>
			<p>
				This method thus breaks the relationship between the variable and the target, and observes the subsequent drop in the models quality (in term of the chosen set of model quality metrics). This methodology is not restricted to only random forests and can be used in any model (its agnostic).
			</p>
			<p>
			One shortcoming of this procedure is that it can be affected by multicolinearity. Suppose we have two very important and also very correlated features in the model. If one of these features is shuffled the model would still perform well as the other variable is still present. This would affect both variables almost identically, leading to an extremely low feature importance for two important features. See https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance_multicollinear.html#sphx-glr-auto-examples-inspection-plot-permutation-importance-multicollinear-py 
			</p>

			<h4>
				Mean Decrease in Impurity
			</h4>
			<p>
				Another common way of measuring the importance of a feature is by directly observing the mean decrease in impurity in the nodes using this particular feature in all trees in the forest. This procedure is quite intuitive and straightforward, but it has two major limitations that must be taken into consideration.
			</p>
			<p>
				Firstly MDI feature importance has a bias towards high cardinality features (such as numerical features). Low cardinality features such as binary or categorical features would have their effects underestimated leading to a biased comprehension of the model. The second limitation is that this method only tels which variable was important during training, which some times is not equivalent to the importance of the features in the test set (during the predictions), as there is no way of measuring the decrease in impurity in such cases.
			</p>

			<h4>
				Avarage minimal depth
			</h4>
			<p>
				This is less know metric, but can be useful sometimes. The idea behind evaluating the importance of a feature by its average minimal depth in the random forest (i.e. the average depth of the first appearance of this feature on each tree of the forest) is that the first splits in a tree are the most impactfull ones, as they split larger portions of the population and hugely affect the quality of the model (if the first split is bad the whole tree suffers, if the last split in a branch of a tree of depth 100 is bad then only this small subset is affected).
			</p>
			<p>
			A very modern variation to the Minimal Depth feature importance is the Surrugate Minimal Depth. This method consist of using the minimal defined by the first appearance not only to primary split variables but also to the occurrence of surrogate variables. This methodology is said "Surrogate minimal depth as an importance measure for variables in random forests "(https://academic.oup.com/bioinformatics/article/35/19/3663/5368013) to lead to better empirical estimation of the feature importance than the normal minimal depth approach.
			</p>
			<p>
			As a closing note to this section, its worth porting out that the feature importance metrics are related to the model, not the features themselves (i.e. the importance is not of the feature itself, but to its influence on your model). <b>A bad feature can be very important to a bad model</b>.
			</p>

			<!--
			<h3>
				Relation to k-nearest neighbor algorithm
			</h3>
			<p>
			There is an interesting relationship between random forests and the KNN algorithm pointed by Lin and Jeon in "". The authors showed that both algorithms have a weighted neighborhood scheme. Let X be the features available and y be the corresponding target, a weighted neighborhood model aims to make predictions $\hat{y}$ for new points $x'$ by looking into its nearest neighbors, given by a weight function W:
			$$\hat{y} = \sum_{i=1}^n W(x_i, x') y_i$$
			where the weight function $W(x_i, x')$ is non-negative and represents the weight of the  i-th training point relative to $x'$. In the KNN the weight function is simply given by $W(x_i, x') = 1/k)$ if $x_i$ is one of the $k$ nearest neighbors (in some predefined metric system) closest to $x'$, and 0 otherwise.
			</p>
			<p>
			In decision trees the weights are also of the form $W(x_i, x')=(1/k)$, but $k$ here represents the points that are in the same leaf than $x'$. As a random forests combines the predictions of many decision trees, the resulting weight function of a random forest will be just the average of the weights of each of the trees:
			$$\hat{y} = \frac{1}{m} \sum+{j=1}^m \sum_{i=1}^n W_j (x_i, x') y_i$$
			where $m$ is the number of trees in the forest. Of course the neighborhood structure in random forest will be structurally very complex (as it adapts from the complexities of the training set). Its shown by the authors that the local shape of the neighborhood function adapts to the local importance of each feature (giving increased weights for important features locally)
		</p>
			-->


			<h3>
				Limitations
			</h3>
			<p>
				As discussed extensively, the major shortcomming of the random forest algorithm is the lack of interpretability. This limitation can be crippling when there is a considerable risk associated with the usage of the model, and decisions cannot be made completely blindly.
			</p>
			<p>
				There are many ways of trying to recover some (or all) interpretability of the RF model. The methods to extract he individual importance of each feature are powerful , but have some caveats that must be considered and represents only estimates of the importance of the features that can be biased in many cases.
			</p>
			<p>
			There are some modern techniques such as SHAP or LIME that try to approximate the final decision hyperplanes with locally interpretable regressors. These methods, while useful, also have limitations and possible biases. Its also proved that SHAP analysis a can be arbitrarily biased without affecting the performance of the model (this can be invisible to common validation techniques and may open the door for exploits in the model).
			</p>
			<p>
			The Born Again algorithm is very recent development and is a really interesting alternative to approach this problem. It offers a seemingly perfect solution: to summarize the entire random forest in a single decision tree that is as complex as the most complex tree in the forest, This solution represents an NP-hard problem and may not converge in a sensible time in some cases, so its a nice tool but it does not solve all problems.
			</p>
			<p>
			Another limitation of random forests is that the presence of multiple categorical variables the resulting ensemble may not perform better than a then base learner as show by Piryonesi <i>et al</i> in "The Application of Data Analytics to Asset Management: Deterioration and Climate Change Adaptation in Ontario Roads"
			</p>



			<a name="CODE">
      </a>
      <h2 class="section">
       3. Applications
      </h2>
      <p section
      </p>
       <span class="alert">
        Click on the panel bellow to start/stop the simulation.
       </span>
       (In some browsers is necessary to click twice to unpause because I screwed the html in some unknown way)
      </p>
      <div class="sketch-holder" id="c1">
      </div>
      <p>
       Other canvas
      </p>
      <div class="sketch-holder" id="c2">
      </div>

      <p>
       Code:
      </p>
<pre>
{% include 'sketch.js' %}
</pre>

    </div>
   </div>
  </div>
  <hr/>
	<!--load d3 images-->
	<script src="feature_space_split.js">
	</script>
	<script src="decision_tree.js">
	</script>
  <script src="../../js/script.js"> <!--TARGET-->
  </script>
  <!-- Bootstrap + JQuery -->
  <script crossorigin="anonymous" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" src="https://code.jquery.com/jquery-3.3.1.slim.min.js">
  </script>
  <script crossorigin="anonymous" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js">
  </script>
  <script crossorigin="anonymous" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js">
  </script>
  <script crossorigin="anonymous" src="https://kit.fontawesome.com/9ead9d8df4.js">
  </script>
  <!-- mathjax -->
  <script async="" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6">
  </script>
  <script>
   MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        svg: {
          fontCache: 'global'
        }
      };
  </script>
 </body>
</html>

