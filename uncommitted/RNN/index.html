<!DOCTYPE html>
<html lang='en'>
	<head>

		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, intial-scale=1">

		<!-- Name on tab -->
		<title>LSTM</title>

		<!-- Google Fonts -->
		<link href="https://fonts.googleapis.com/css?family=Cormorant+Garamond:400,700&display=swap" rel="stylesheet"> 

		<!-- Custom CSS -->
		<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
		<link rel="stylesheet" href="../../css/notebook.css"> <!--TARGET-->
		<link rel="stylesheet" href="../../css/pygments.css"> <!--TARGET-->
		<link rel="stylesheet" href="../../css/style.css"> <!--TARGET-->

		<script src="https://cdn.jsdelivr.net/npm/p5@1.1.9/lib/p5.js"></script>

		<!-- Custom p5js animations -->
		<script src="sketch.js"></script>

    <!-- mathjax -->
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        svg: {
          fontCache: 'global'
        }
      };
    </script>

	</head>

	<body>
		<!-- Navigation bar -->
		<nav class="navbar navbar-expand-lg navbar-dark bg-dark fixed-top">
			<a class="navbar-brand" href="../.." style="font-weight:bold;">Ariel Yssou</a> <!--TARGET-->
			<button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
				<span class="navbar-toggler-icon"></span>
			</button>
			<div class="collapse navbar-collapse" id="navbarSupportedContent">
				<ul class="navbar-nav mr-auto">
					<li class="nav-item">
						<a class="nav-link" href="../.." style="font-weight:bold;">Home</a><!--TARGET-->
 
					</li>
					<li class="nav-item active">
						<a class="nav-link" href="../../blog" style="font-weight:bold;">Blog <span class="sr-only">(current)</span></a><!--TARGET-->
					</li>
					<li class="nav-item">
						<a class="nav-link" href="../../about" style="font-weight:bold;">About</a><!--TARGET-->
					</li>
				</ul>
			</div>
			<div class="collapse navbar-collapse" id="navbarSupportedContent">
				<ul class="navbar-nav ml-auto">
					<li class="nav-item">
						<a class="nav-link text-nowrap" href="https://github.com/ArielYssou" target="_blank"><i class="fab fa-github fa-lg"></i></a>
					</li>
					<li class="nav-item">
						<a class="nav-link text-nowrap" href="https://www.linkedin.com/in/ariel-yssou-oliveira-f-2a07a5b5/" target="_blank"><i class="fab fa-linkedin fa-lg"></i></a>
					</li>
				</ul>
			</div>
		</nav>
		<!-- End of navigation bar -->
		
		<div class="container post_body">
			<div class="row post_struct">
				<div class="col-ld-12">
					<div class="head_container">
						<img src="./cover_image.png" class="head_image no_blur">
						<div class="head_middle">
							<div class="head_text"><b>Long Short-Term Memory</b></div>
						</div>
					</div>

					<p> In this post we will tackle the task of time series forecasting with <b>Long Short-Term Memory (LSTM)</b> neural networks using the "Historical Hourly Weather Data 2012-2017" dataset. The task of predicting the weather is a challenging one, more so without using any specific models or insights from know meteorological models, but we will try to approach this problem armed only with machine learning! </p>

					<!-- TOC -->
 <table class="TOC">
  <tr><th>Contents</th></tr>
	<tr><td><a href="#RNN">1. Recurrent Neural Networks</a></td></tr>
	<tr><td><a href="#LSTM">2. Long Short Term Memory</a>	</td></tr>
	<tr><td><a href="#CODE">3. Implementation</a></a></td></tr>
</table> 
<br></br>

					<!-- Intro -->
					<a name="RNN"></a>
					<h2 class="section">1. Recurrent Neural Networks</h2>
					<p>Our objective will be to build a model that is able to predict the future elements of a given sequence $x_0, x_1, \dots, x_t$. The first instinct might be to use a "traditional" neural network with sequential fully connected layers such as this:</p>
					<div class="sketch-holder" id="c1"></div><br>
					<!--NN scheme-->

					<p>While effective this type of architecture does not exploit the sequential structure of the data and thus yields poor generalization. The key feature that these networks miss is some sort of "memory". Indeed, at step $t$ the state $h_t$ of a neuron in the hidden layer can be written as:
					$$h_t = f( \vec{w}_t . \vec{x}_t),$$
					where $\vec{w}_t$ are weights (bias is omitted for simplicity) and $f$ a generic activation function. There is no explicit dependence on the "historic" of the data, as they only affect the way the weights are trained (thus being extremely sensible to the starting point of the data sequence). One way we can try to solve this limitation is by introducing an explicit dependence on the past state of the neuron:
					$$
					h_t = f(\vec{w}_t . \vec{x}_t + \vec{u}_t . \vec{h}_{t-1}),
					$$
					where $\vec{u}_t$ is a new set of parameters that control how much of the past information (or "memory") is used. This transforms the expression of the state of the neuron into a <i>recurrence</i> relation as $h_t$ now depends on $h_{t-1}$. This is one of the ideas that inspired the formulation of <b>Recurrent Neural Networks (RNN)</b> (not to be confused with <i>Recursive neural nets</i> that share the same acronym and sound similar, <a href="https://en.wikipedia.org/wiki/Recursive_neural_network" target="_">but have differences</a>). As the hidden units use their own past output to proceed, the same parameters are used over and over again. This <i>parameter sharing</i> can be loosely interpreted as the network keeping what it has already leaned previously and is one of the reasons why RNN are best suited for sequential data. The structure of a RNN can be illustrated as:</p>
					<!-- rnn SCHEME -->
					<div class="sketch-holder" id="c2"></div>
					<p>The representation on the right is know as the <i>unfolded</i> computational graph of a RNN, which is just a fancy name for expanding a recurrence relation. The unfolded network can be seen as just a sequence of copies of a single layer, applied over the entire sequence of $x$. This is the general structure of a RNN but there are many variants to this scheme. While explaining them all is far beyond the scope of this text, you can read more about then <a href="http://www.deeplearningbook.org/contents/rnn.html#pfb" target='_'>here</a> if you are interested.</p>
					<!-- Conclusion RNN -->
					<p>Although "traditional" RNNs are quite powerful they have some limitations. While there is no theoretical limit to a RNN memory, practice shows that its hard to learn long term dependencies with (see <a href = "https://ieeexplore.ieee.org/document/279181" target="_">Bengio <i>et al</i></a>) as their effects are heavily "diluted" as the recurrence is iterated. This limitation arises from the recurrence itself as the network is as deep as the length of the sequence and thus the same weights are used several times. If we were dealing, for example, with a time series of 100 steps; the recurrence will apply the same weights $100$ times, thus the final state $h_{100}$ will depend on the weight matrix $ W^{100} $. The gradients (used in gradient descent) linked to eigenvalues slightly different than 1 will wither vanish or explode, thus making training a hard task. <b>LSTM</b> is one popular RNN model that tackles this problem and is specially built to deal with both long and short term dependencies.</p>
					<!-- LTSM -->
					<a name="LSTM"></a><h2 class="section">2. Long Short Term Memory</h2>
					<p>LSTM addresses the problem of long term memory by making several modifications to the vanilla RNN architecture. Indeed its structure is far more complex:</p>
					<div class="sketch-holder" id="c3"></div>
					<p>I cannot even dream of writing a better explanation on LSTMs than Colah in his amazing blog post, but I'll try to explain then in a different way.</p>

					<p>The idea is to combine a short memory term with a long memory term
					$$h_t = (``Short\ memory") \times (``Long\ memory") \equiv S_t + L_t.$$</p>
					<p>The short memory term $S_t$ will only combine the input with the very last state of the cell. This short memory will be learned via a usual neural net layer with a $\sigma$ activation function.
					$$S_t = \sigma ( w_s [h_{t-1}, x_t] + b_s ),$$
					where $w_s$ and $b_s$ are the weights and bias of this layer.The $[,]$ is the concatenation operator. The long memory term $L_t$ will be responsible of carrying the memory of the cell, forgetting any currently held information (old information") and adding new information based on the current input and state. We can think of it like:</p>
					<p>
					$$
					\begin{eqnarray*}
					L_t &=& (``Forget\ rate") . (``Old\ information") + (``Acceptance\ rate") . (``New\ information")\\
					\, &\equiv& f_t L_{t-1} + a_t \tilde{L}_{t-1}.
					\end{eqnarray*}$$
					</p>
					<p>The old information is the last value of the memory which is equivalent to $L_{t-1}$. Both the "forget rate" $f_t$ , "acceptance rate" $a_t$ and "new information" $\tilde{L}_t$ must be learned. This is done via a usual neural net layers:
					$$\begin{eqnarray*}
					f_t &=& \sigma ( w_f . [h_{t-1}, x_t] + b_f ),\\
					a_t &=& \sigma ( w_a . [h_{t-1}, x_t] + b_a ),\\
					\tilde{L}_t &=& \tanh ( w_l . [h_{t-1}, x_t] + b_l ),\\
					\end{eqnarray*}$$
					where all the $w$ are separate sets of weights and $b$ the biases that need to be learned. In order to "smoothly normalize" the long memory term (as it can exceed unity as its is the sum of two things that can be at maximum 1) the LSTM model applies a $tanh$ to it. This leads us to the final form of the state of the LSTM cell:
					$$\boxed{h_t = S_t \tanh(L_t)}.$$</p>
					<a name="CODE"></a><h2 class="section">3. Implementation</h2>
					{% include 'weather.html' %}
				</div>
			</div>
		</div>

		<hr/>
		<script src="../../js/script.js"> </script><!--TARGET-->
		<!-- Bootstrap + JQuery -->
		<script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
		<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
		<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>

		<script src="https://kit.fontawesome.com/9ead9d8df4.js" crossorigin="anonymous"></script>



	</body>
</html>
